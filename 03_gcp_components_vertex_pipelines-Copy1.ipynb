{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8184cc-3583-4187-b58b-02c48d08ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57301cb-c630-41ab-90a0-30442ea38e06",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 02 - Vertex Pipelines using Custom KubeFlow Components\n",
    "\n",
    "## Overview \n",
    "\n",
    "This notebook shows how to use Kubeflow components to build a custom regression workflow on `Vertex AI Pipelines`.\n",
    "\n",
    "You will build a pipeline in this notebook that looks like this:\n",
    "\n",
    "<img src=\"img/vertex-pipelines.png\" width=\"80%\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Notebook Objective\n",
    "\n",
    "In this notebook, you will learn to use `Vertex AI Pipelines`, `KubeFlow Components` and `Google Cloud Pipeline Components` to build a `custom` tabular regression model. In the pipeline we  will orchestrate data creation, data processing, model training and evaluation, and model deployment. We'll also see how to send payloads the endpoint deployed and how to run batch predition jobs.\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- `BigQuery`\n",
    "- `Vertex AI Pipelines`\n",
    "- `Google Cloud Pipeline Components`\n",
    "- `Vertex AI Model`\n",
    "- `Vertex AI Model Registry`\n",
    "- `Vertex AI Metadata`\n",
    "- `Vertex AI Endpoint`\n",
    "\n",
    "The steps performed in this notebook include:\n",
    "\n",
    "1. [Load Configuration settings from the setup notebook](#Load-Configuration-settings-from-the-setup-notebook)\n",
    "1. [Vertex Pipelines Introduction](#Vertex-Pipelines-Introduction)\n",
    "1. [Create a KFP Pipeline](#Create-a-KFP-Pipeline)\n",
    "    1. [Create a dataset in BigQuery](#Step-1:-Create-a-dataset-in-BigQuery)\n",
    "    1. [Transform the Data](#Step-2:-Transform-the-Data)\n",
    "    1. [Train and Evaluate our custom Regression Model](#Step-3:-Train-and-Evaluate-our-custom-Regression-Model)\n",
    "    1. [Upload Model to Vertex AI and Deploy Endpoint](#Step-4:-Upload-Model-to-Vertex-AI-and-Deploy-Endpoint)\n",
    "1. [Compile the KFP Pipeline](#Compile-the-KFP-Pipeline)\n",
    "1. [Execute the KFP Pipeline using Vertex AI Pipelines](#Execute-the-KFP-Pipeline-using-Vertex-AI-Pipelines)\n",
    "1. [Inspect Experiments](#)\n",
    "1. [Online Predictions](#)\n",
    "1. [Batch Preditions](#)\n",
    "\n",
    "\n",
    "The Google Cloud Components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform) and the KubeFlow Compoenents are [documented here](https://www.kubeflow.org/docs/components/pipelines/v1/sdk-v2/python-function-components/)\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this workshop we'll use the **public datase**t [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) for demonstration purposes. The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes. The objective will be to build a model to predict \"MPG\" (Miles per Gallon).\n",
    "\n",
    "Check notebook  `01_exploratory_data_analysis.ipynb` for further details of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f8fe7-3013-4671-b5ad-fc5931a6251f",
   "metadata": {},
   "source": [
    "## Load Configuration settings from the setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed77f6d-ebae-413d-b2dc-ddd0cd85252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our configurations from notebook 00_environment_setup.ipynb\n",
    "from src.config import config\n",
    "\n",
    "PROJECT_ID = config['PROJECT_ID']\n",
    "REGION = config['REGION']\n",
    "ID = config['ID']\n",
    "BUCKET_NAME = config['BUCKET_NAME']\n",
    "GCS_DATA_URI = config['GCS_DATA_URI']\n",
    "BQ_DATASET_URI = config['BQ_DATASET_URI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a110447-b83e-42c2-923d-36ea9d314329",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f719f7ec-8eaf-4892-9eef-b920c3574ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Import the Vertex AI Python SDK \n",
    "from google.cloud import aiplatform as aip\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import google.auth\n",
    "from google.cloud import storage\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "# kfp sdk, to create the Vertex AI Pipelines\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, Model, Output, Metrics, ClassificationMetrics, component, OutputPath, InputPath)\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "\n",
    "# TensorFlow model building libraries.\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Custom Modules\n",
    "from src.helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb9ee9-013f-492d-99a5-2244a62eb43d",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92536907-36a6-4a7c-a96f-f5f589357180",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a9238-094d-45aa-849f-76170342ef45",
   "metadata": {},
   "source": [
    "## Create Custom Training Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e3f43-0464-480a-8ccc-082522f4e321",
   "metadata": {},
   "source": [
    "Train and deploy your model on Google Cloud's Vertex AI platform.\n",
    "\n",
    "To train your BERT classifier on Google Cloud, you will you will package your Python training scripts and write a Dockerfile that contains instructions on your ML model code, dependencies, and execution instructions. You will build your custom container with Cloud Build, whose instructions are specified in `cloudbuild.yaml` and publish your container to your Artifact Registry. This workflow gives you the opportunity to use the same container to run as part of a portable and scalable [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) workflow. \n",
    "\n",
    "\n",
    "You will walk through creating the following project structure for your ML mode code:\n",
    "```\n",
    "|--/container\n",
    "   |--/trainer\n",
    "      |--__init__.py\n",
    "      |--model.py\n",
    "      |--task.py\n",
    "   |--Dockerfile\n",
    "   |--cloudbuild.yaml\n",
    "   |--requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236ead7-5f66-49aa-9cce-a689afbe796c",
   "metadata": {},
   "source": [
    "### Step 1: Write model.py training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef801c6e-a6f5-4be9-aca3-7f05c348aefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'container'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DIR = f\"container\"\n",
    "MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ebd943-ec67-4b4c-a776-16727d2e81ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing container/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_DIR}/trainer/model.py\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "bqclient = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def download_table(bq_table_uri: str):\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "    )\n",
    "    \n",
    "    return rows.to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "\n",
    "def build_and_compile_model(norm):\n",
    "        model = keras.Sequential([\n",
    "            norm,\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "        return model\n",
    "    \n",
    "\n",
    "def train_model(params):\n",
    "    raw_dataset = download_table(params['data-dir'])\n",
    "    raw_dataset.rename(columns = {\n",
    "        'mpg':'MPG',\n",
    "        'cyl':'Cylinders',\n",
    "        'dis':'Displacement',\n",
    "        'hp': 'Horsepower',\n",
    "        'weight': 'Weight',\n",
    "        'accel': 'Acceleration',\n",
    "        'year': 'Model Year',\n",
    "        'origin': 'Origin'}, inplace = True)\n",
    "\n",
    "    # Get data in shape\n",
    "    dataset = raw_dataset.copy()\n",
    "    dataset.tail()\n",
    "    dataset = dataset.dropna()\n",
    "    dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "    dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\n",
    "    train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    train_features = train_dataset.copy()\n",
    "    test_features = test_dataset.copy()\n",
    "    train_labels = train_features.pop('MPG')\n",
    "    test_labels = test_features.pop('MPG')\n",
    "\n",
    "    # Create model\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    first = np.array(train_features[:1])\n",
    "    horsepower = np.array(train_features['Horsepower'])\n",
    "    horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
    "    horsepower_normalizer.adapt(horsepower)\n",
    "\n",
    "\n",
    "    dnn_model = build_and_compile_model(normalizer)\n",
    "    dnn_model.summary()\n",
    "\n",
    "    history = dnn_model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        validation_split=0.2,\n",
    "        verbose=0, epochs=100\n",
    "    )\n",
    "\n",
    "    test_results = {}\n",
    "\n",
    "    test_results['dnn_model'] = dnn_model.evaluate(\n",
    "        test_features,\n",
    "        test_labels,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Log metrics\n",
    "    metrics_training = {metric: values[-1] for metric, values in history.history.items()}\n",
    "    metrics.log_metric('loss', metrics_training['loss'])\n",
    "    metrics.log_metric('val_loss', metrics_training['val_loss'])\n",
    "    model.uri = bucket\n",
    "    model.metadata['loss'] = metrics_training['loss']\n",
    "    model.metadata['val_loss'] = metrics_training['val_loss']\n",
    "    model.metadata['pipeline'] = pipeline_name\n",
    "\n",
    "    # Save the model to GCS\n",
    "    dnn_model.save(params['model-dir'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599fd5d-648d-4f83-b40d-5f5f35eb5009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8fb82fc-a97f-4969-8b08-88ca5607909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing container/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {MODEL_DIR}/trainer/task.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Vertex custom container training args. These are set by Vertex AI during training but can also be overwritten.\n",
    "    parser.add_argument('--model-dir', dest='model-dir',\n",
    "                        default=os.environ['AIP_MODEL_DIR'], type=str, help='GCS URI for saving model artifacts.')\n",
    "    \n",
    "    parser.add_argument('--data-dir', dest='data-dir',\n",
    "                        default=os.environ['AIP_TRAINING_DATA_URI'], type=str, help='BQ URI where the data is')    \n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    params = args.__dict__\n",
    "    \n",
    "    \n",
    "    model.train_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02e455-11ab-4084-b08f-c7bd34334141",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b959a51-bc11-44aa-86fe-fe2a2048f8d7",
   "metadata": {},
   "source": [
    "## Vertex Pipelines Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27cd415-58cf-41a1-8b62-94827d6495bd",
   "metadata": {},
   "source": [
    "[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner, and storing your workflow's artifacts using Vertex ML Metadata. By storing the artifacts of your ML workflow in Vertex ML Metadata, you can analyze the lineage of your workflow's artifacts â€” for example, an ML model's lineage may include the training data, hyperparameters, and code that were used to create the model.\n",
    "\n",
    "You can build your Pipelines using the battle-tested and easy-to-use `KubeFlow Pipelines (KFP) SDK` or ` TensorFlow Extended (TFX) SDK`. \n",
    "\n",
    "Within your Vertex Pipeline with `KubeFlow Pipelines (KFP) SDK` you can use either your own custom components using `KubeFlow Components` or already-built compontents using `Google Cloud Pipeline Components`.\n",
    "\n",
    "The Google Cloud Components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform). \n",
    "\n",
    "The KubeFlow Compoenents are [documented here](https://www.kubeflow.org/docs/components/pipelines/v1/sdk-v2/python-function-components/)\n",
    "\n",
    "<img src=img/vertex-pipelines-def.png width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a7e96-de39-4511-b51c-b2e84e9e7dfc",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48781f94-263c-49dd-b7f2-261b65084201",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561d229-ec9e-4e05-9086-6bdd533a009a",
   "metadata": {},
   "source": [
    "## Create a KFP Pipeline\n",
    "\n",
    "To address your business requirements and get your higher performing model into production to deliver value faster, you will define a pipeline using the [**Kubeflow Pipelines (KFP) V2 SDK**](https://www.kubeflow.org/docs/components/pipelines/sdk/v2/v2-compatibility) to orchestrate the training and deployment of your model on [**Vertex Pipelines**](https://cloud.google.com/vertex-ai/docs/pipelines) below.\n",
    "\n",
    "The pipeline consists of four custom KFP V2 components:\n",
    "\n",
    "* `create_bq_dataset_op`: Create BigQuery Dataset from data stored as csv in Google Cloud Storage.\n",
    "\n",
    "* `prepare_datasets_op`: gets the required data and transform it for the training requirements.\n",
    "\n",
    "* `train_evaluate_model_op`: Trains our regression model to predig MPG and  evaluates the trained model and makes the decision whether to deploy the model and create an endpoint based on a passed threshold.\n",
    "\n",
    "\n",
    "*  `deploy_model_endpoint Custom Component`: Uploads trained model to Vertex AI and creates a Google Cloud Vertex Endpoint resource that maps physical machine resources with your model to enable it to serve online predictions. Online predictions have low latency requirements; providing resources to the model in advance reduces latency. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99265ece-aa4f-4b95-a4d0-21a2fffefe70",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d84833-fb68-495a-a814-8de308401b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install= ['google-cloud-bigquery[bqstorage,pandas]'],\n",
    "    output_component_file=\"prepare_datasets_op.yml\"\n",
    ")\n",
    "def prepare_datasets_op(\n",
    "    bq_dataset: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    ")-> NamedTuple(\"Outputs\", [(\"train_dataset_path\", str), (\"test_dataset_path\", str)]):\n",
    "    \n",
    "    # print(f\"Input dataset is: {bq_dataset}\")\n",
    "    # print(f\"Input dataset is: {bq_dataset.uri}\")\n",
    "    # print(f\"Input dataset is: {bq_dataset.location}\")\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import os\n",
    "    \n",
    "    \n",
    "    \n",
    "    bqclient = bigquery.Client()\n",
    "    \n",
    "    def download_table(bq_table_uri: str):\n",
    "        prefix = \"bq://\"\n",
    "        if bq_table_uri.startswith(prefix):\n",
    "            bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "        table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "        rows = bqclient.list_rows(\n",
    "            table,\n",
    "        )\n",
    "        return rows.to_dataframe(create_bqstorage_client=False)\n",
    "    \n",
    "    \n",
    "    raw_dataset = download_table(bq_dataset)\n",
    "    raw_dataset.rename(columns = {\n",
    "        'mpg':'MPG',\n",
    "        'cyl':'Cylinders',\n",
    "        'dis':'Displacement',\n",
    "        'hp': 'Horsepower',\n",
    "        'weight': 'Weight',\n",
    "        'accel': 'Acceleration',\n",
    "        'year': 'Model Year',\n",
    "        'origin': 'Origin'}, inplace = True)\n",
    "\n",
    "    # Get data in shape\n",
    "    dataset = raw_dataset.copy()\n",
    "    dataset.tail()\n",
    "    dataset = dataset.dropna()\n",
    "    dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "    dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\n",
    "    train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    \n",
    "    train_dataset_path = dataset_train.path\n",
    "    test_dataset_path = dataset_test.path\n",
    "    logging.info(f\"Dataset Train is be stored in: {dataset_train.path}\")\n",
    "    logging.info(f\"Dataset Test is be stored in: {dataset_test.path}\")\n",
    "    \n",
    "    train_dataset.to_csv(dataset_train.path, index=False)\n",
    "    test_dataset.to_csv(dataset_train.path, index=False)\n",
    "    \n",
    "    return (train_dataset_path, test_dataset_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a56bb-c851-481c-9b15-32e9897a1519",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09e711-4b50-425c-999d-d81888a6eaf2",
   "metadata": {},
   "source": [
    "### Custom Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d55ccea5-493d-4e62-b1f0-89840a88f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"tensorflow/tensorflow:2.10.0\",\n",
    "    packages_to_install=['pandas'],\n",
    "    output_component_file=\"train_evaluate_model_op.yml\"\n",
    ")\n",
    "def train_evaluate_model_op(\n",
    "    dataset_train: Input[Dataset],\n",
    "    dataset_test: Input[Dataset],\n",
    "    threshold_dict_str: str,\n",
    "    pipeline_name: str,\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model],\n",
    "    model_uri: OutputPath(str),\n",
    ") -> NamedTuple(\n",
    "        'Outputs',[\n",
    "            ('loss', float),\n",
    "            ('val_loss', float),\n",
    "            ('dep_decision', str),\n",
    "            ('model_path', str)\n",
    "        ]):\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import tensorflow as tf\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    import json\n",
    "    \n",
    "    logging.info(f\"Train Dataset path {dataset_train.path}\")\n",
    "    logging.info(f\"Test Dataset path {dataset_test.path}\")\n",
    "    logging.info(f\"train dataset - {dataset_train.path}\")\n",
    "    \n",
    "    train_dataset = pd.read_csv(dataset_train.path)\n",
    "    test_dataset = pd.read_csv(dataset_train.path)\n",
    "    train_features = train_dataset.copy()\n",
    "    test_features = test_dataset.copy()\n",
    "    train_labels = train_features.pop('MPG')\n",
    "    test_labels = test_features.pop('MPG')\n",
    "\n",
    "    \n",
    "    # Create model\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    first = np.array(train_features[:1])\n",
    "    horsepower = np.array(train_features['Horsepower'])\n",
    "    horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
    "    horsepower_normalizer.adapt(horsepower)\n",
    "\n",
    "    def build_and_compile_model(norm):\n",
    "        model = keras.Sequential([\n",
    "            norm,\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        metrics_tf = [tf.metrics.MeanAbsoluteError(), tf.metrics.MeanAbsolutePercentageError(), \n",
    "               tf.metrics.MeanSquaredError()]\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error', \n",
    "                      optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                      metrics=metrics_tf\n",
    "                     )\n",
    "        return model\n",
    "\n",
    "    dnn_model = build_and_compile_model(normalizer)\n",
    "    dnn_model.summary()\n",
    "\n",
    "    history = dnn_model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        validation_split=0.2,\n",
    "        verbose=0, epochs=100\n",
    "    )\n",
    "\n",
    "    \n",
    "    loss_test, mae, mape, mse = dnn_model.evaluate(test_features,\n",
    "                                                       test_labels,\n",
    "                                                       verbose=0\n",
    "                                                      )\n",
    "    \n",
    "    logging.info(f\"TEST METRICS: {loss_test}, {mae}, {mape}, {mse}\")\n",
    "\n",
    "    # Log metrics\n",
    "    metrics_training = {metric: values[-1] for metric, values in history.history.items()}\n",
    "    metrics.log_metric('loss', metrics_training['loss'])\n",
    "    metrics.log_metric('val_loss', metrics_training['val_loss'])\n",
    "    metrics.log_metric('test_loss', loss_test)\n",
    "    metrics.log_metric('test_mae', mae)\n",
    "    metrics.log_metric('test_mape', mape)\n",
    "    metrics.log_metric('test_mse', mse)\n",
    "  \n",
    "    \n",
    "    model.metadata['loss'] = metrics_training['loss']\n",
    "    model.metadata['val_loss'] = metrics_training['val_loss']\n",
    "    model.metadata['test_loss'] = loss_test\n",
    "    model.metadata['pipeline'] = pipeline_name\n",
    "\n",
    "    # Save the model to GCS\n",
    "    dnn_model.save(model.path)\n",
    "\n",
    "    threshold_dict = json.loads(threshold_dict_str)\n",
    "    kpi_decision = float(threshold_dict['mae'])\n",
    "    \n",
    "        \n",
    "    if mae <= kpi_decision:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(f\"deployment decision is {dep_decision}\")\n",
    "    logging.info(f\"model will be stored in {model.path}\")\n",
    "    \n",
    "    model_uri = model.path\n",
    "\n",
    "    return (metrics_training['loss'], metrics_training['val_loss'], dep_decision, model.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee292b-1eef-4934-b6f3-92796a1a8ca2",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db9ace-a640-476f-87c0-dbae1a2989d7",
   "metadata": {},
   "source": [
    "### Step 4: Upload Model to Vertex AI and Deploy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e9dca3-3339-4d53-9b89-0d0301f18eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  base_image=\"python:3.9\",\n",
    "  packages_to_install=['google-cloud-aiplatform'],\n",
    "  output_component_file=\"upload_model_op.yml\"\n",
    ")\n",
    "def deploy_model_endpoint_op(\n",
    "    model: Input[Model],\n",
    "    region: str,\n",
    "    model_name: str,\n",
    "    pipeline_name: str,\n",
    "    project_id: str,\n",
    "    serving_container_image_uri: str,\n",
    "    machine_type: str,\n",
    "    model_display_name: str,\n",
    "    endpoint_display_name:str,\n",
    "    vertex_model: Output[Artifact]\n",
    "):\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as aip\n",
    "        \n",
    "    labels={'pipeline': str(pipeline_name),\n",
    "            'country': 'germany'}\n",
    "   \n",
    "    aip.init(project=project_id, location=region)\n",
    "\n",
    "    logging.info(f\"Model URI {model.uri}\")\n",
    "    logging.info(f\"Model Name: {model_name}\")\n",
    "    logging.info(f\"Model Labels: {labels}\")\n",
    "    logging.info(f\"serving_container_image_uri {serving_container_image_uri}\")\n",
    "    \n",
    "    def create_endpoint(endpoint_display_name, project, region):\n",
    "        endpoints = aip.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(endpoint_display_name),\n",
    "        order_by='create_time desc',\n",
    "        project=project, \n",
    "        location=region,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0]  # most recently created\n",
    "        else:\n",
    "            endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=endpoint_display_name, project=project, location=region\n",
    "        )\n",
    "        return endpoint\n",
    "    \n",
    "    # Create Endpoint     \n",
    "    endpoint = create_endpoint(endpoint_display_name, project_id, region) \n",
    "    \n",
    "    \n",
    "    model_upload = aip.Model.upload(  \n",
    "        display_name=model_name,\n",
    "        artifact_uri=model.uri,\n",
    "        description='Regression model for fuel prediction',\n",
    "        labels=labels,\n",
    "        serving_container_image_uri=serving_container_image_uri\n",
    "        )\n",
    "    \n",
    "    logging.info(f'Input Endpoint {endpoint}')\n",
    "    model_deploy = model_upload.deploy(\n",
    "        machine_type=machine_type, \n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=model_display_name,\n",
    "    )\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = model_deploy.resource_name\n",
    "    logging.info(model_deploy.resource_name)\n",
    "    \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd4b7c-1b4f-47f3-9105-b638651ecf12",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a73721-eee4-4f0c-a772-76583d451090",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile the KFP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e884bf3-5834-4374-9695-35a44479f947",
   "metadata": {},
   "source": [
    "#### Define Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "120bb60e-dac5-4e6b-a72f-86d807717463",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "USER = 'add-your-name-lowercase'\n",
    "USER = 'gabriela'\n",
    "if USER == 'add-your-name-lowercase':\n",
    "    USER = 'unknown'\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = \"fuel-model\"\n",
    "EXPERIMENT_DESCRIPTION = \"Fuel prediction pipeline\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/{USER}\"\n",
    "\n",
    "## Important the pipeline name always has the timestamp as part of the name.\n",
    "PIPELINE_NAME = f\"{EXPERIMENT_NAME}-{TIMESTAMP}\"\n",
    "PIPELINE_PACKAGE_PATH = f'{PIPELINE_NAME}-path.json'\n",
    "\n",
    "\n",
    "MODEL_NAME = EXPERIMENT_NAME\n",
    "THRESHOLD_DICT_STR = '{\"mae\": 10000}'\n",
    "\n",
    "DATASET_NAME = 'fuel_dataset'\n",
    "TABLE_NAME = 'main'\n",
    "MODEL_NAME = 'fuel-prediction'\n",
    "DATA_GCS_DIR = GCS_DATA_URI\n",
    "\n",
    "\n",
    "ENDPOINT_DISPLAY_NAME = 'fuel-endpoint'\n",
    "SERVING_CONTAINER_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'\n",
    "ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "\n",
    "LABELS = {\n",
    "    'creator': USER,\n",
    "    'workflow': 'fuel-prediction',\n",
    "    'type': 'regression'}\n",
    "\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_NAME,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    experiment_description=\"Fuel prediction pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8691e67-66c8-4021-80b4-af004b4619bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=PIPELINE_NAME\n",
    ")\n",
    "def pipeline(\n",
    "    gcs_uri: str,\n",
    "    dataset_name: str,\n",
    "    threshold_dict_str: str,\n",
    "    user:str,\n",
    "    managed_dataset_name: str,\n",
    "    endpoint_display_name: str,\n",
    "    endpoint_machine_type:str,\n",
    "    table_name: str,\n",
    "    project_id: str,\n",
    "    pipeline_name: str,\n",
    "    serving_container_image_uri:str,\n",
    "    model_name: str,\n",
    "    region: str):\n",
    "\n",
    "    # STEP 1:  Create bq table and dataset\n",
    "    create_bq_dataset_task = create_bq_dataset_op(\n",
    "        gcs_uri=gcs_uri,\n",
    "        project_id=project_id,\n",
    "        dataset_name=dataset_name,\n",
    "        table_name=table_name\n",
    "        ).set_caching_options(True) \\\n",
    "        .set_display_name('create-bq-dataset-table-op')\n",
    "    \n",
    "    \n",
    "    \n",
    "   # STEP 2: Prepare train and test datasets\n",
    "    prepare_data_op = prepare_datasets_op(\n",
    "        #bq_dataset=create_dataset_op.outputs['dataset']\n",
    "        bq_dataset =create_bq_dataset_task.outputs['bq_dataset_uri']\n",
    "    ).set_caching_options(True) \\\n",
    "        .set_display_name('prepare-datasets-op')\n",
    "\n",
    "    # STEP 3: Train and evaluate model\n",
    "    train_evaluate_model_task = train_evaluate_model_op(\n",
    "        dataset_train = prepare_data_op.outputs['dataset_train'],\n",
    "        dataset_test = prepare_data_op.outputs['dataset_test'],\n",
    "        threshold_dict_str=threshold_dict_str,\n",
    "        pipeline_name=pipeline_name\n",
    "    ).set_display_name('training-evaluation-job-op')\\\n",
    "        .set_caching_options(True)\n",
    "\n",
    "    ## Step3: Decision: If model performs according to our threshold, then deploy model and Enp\n",
    "    with dsl.Condition(\n",
    "            train_evaluate_model_task.outputs[\"dep_decision\"] == \"true\",\n",
    "            name=\"deploy_decision\",\n",
    "        ):\n",
    "            # Upload Model, Create a Vertex Endpoint resource and Deploy Model to Endpoint.\n",
    "\n",
    "            deploy_model_endpoint_op(\n",
    "                            model=train_evaluate_model_task.outputs['model'],\n",
    "                            region=region,\n",
    "                endpoint_display_name=endpoint_display_name,\n",
    "                            model_name=model_name,\n",
    "                            pipeline_name=pipeline_name,\n",
    "                            project_id=project_id,\n",
    "                machine_type=endpoint_machine_type,\n",
    "                model_display_name=model_name,\n",
    "                serving_container_image_uri=serving_container_image_uri\n",
    "            ).set_display_name('deploy-model-op').set_caching_options(True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ad6939c-1fa4-405f-8fbf-527e36966561",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ec150-96ef-4f89-8507-4378c58e1f19",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997c21e-2436-4262-b606-cf693d2c1f44",
   "metadata": {},
   "source": [
    "### Execute the KFP Pipeline using Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27876437-0828-44d2-b608-3751051fd2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/525292812449/locations/us-central1/pipelineJobs/fuel-model-20221023204859-20221023205033\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/525292812449/locations/us-central1/pipelineJobs/fuel-model-20221023204859-20221023205033')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/fuel-model-20221023204859-20221023205033?project=525292812449\n",
      "Associating projects/525292812449/locations/us-central1/pipelineJobs/fuel-model-20221023204859-20221023205033 to Experiment: fuel-model\n"
     ]
    }
   ],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=True,\n",
    "    labels=LABELS,\n",
    "    parameter_values={\n",
    "        'gcs_uri': DATA_GCS_DIR,\n",
    "        'dataset_name': DATASET_NAME,\n",
    "        'threshold_dict_str':THRESHOLD_DICT_STR,\n",
    "        'user': USER,\n",
    "        'managed_dataset_name': DATASET_NAME,\n",
    "        'endpoint_display_name': ENDPOINT_DISPLAY_NAME,\n",
    "        'endpoint_machine_type': ENDPOINT_MACHINE_TYPE,\n",
    "        'table_name': TABLE_NAME,\n",
    "        'project_id': PROJECT_ID,\n",
    "        'pipeline_name': PIPELINE_NAME,\n",
    "        'serving_container_image_uri': SERVING_CONTAINER_IMAGE_URI,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'region': REGION\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "job.submit(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b36214-0ad4-4459-85f9-917e957404ca",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57f6e2-a4d8-40e4-bdc3-51f1f47a1a43",
   "metadata": {},
   "source": [
    "## Inspect Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b3bf9aa-d771-497b-acce-5e2f7efa5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiments_data(\n",
    "  experiment_name: str,\n",
    "  project: str,\n",
    "  location: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Get experiments\n",
    "    \"\"\"\n",
    "    aip.init(experiment=experiment_name, project=project, location=location)\n",
    "    experiments_df = aip.get_experiment_df()\n",
    "    return experiments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5fcf265-bd3f-41f3-a8fd-67a800c0730e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>run_type</th>\n",
       "      <th>state</th>\n",
       "      <th>param.endpoint_display_name</th>\n",
       "      <th>param.dataset_name</th>\n",
       "      <th>param.table_name</th>\n",
       "      <th>param.threshold_dict_str</th>\n",
       "      <th>param.region</th>\n",
       "      <th>param.model_name</th>\n",
       "      <th>...</th>\n",
       "      <th>param.managed_dataset_name</th>\n",
       "      <th>param.user</th>\n",
       "      <th>param.endpoint_machine_type</th>\n",
       "      <th>param.project_id</th>\n",
       "      <th>metric.test_loss</th>\n",
       "      <th>metric.loss</th>\n",
       "      <th>metric.test_mse</th>\n",
       "      <th>metric.val_loss</th>\n",
       "      <th>metric.test_mae</th>\n",
       "      <th>metric.test_mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221023204859-20221023205033</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>2.010309</td>\n",
       "      <td>1.679610</td>\n",
       "      <td>6.684991</td>\n",
       "      <td>3.377044</td>\n",
       "      <td>2.010309</td>\n",
       "      <td>9.169966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221023000459</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022234350</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022234031</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022233707</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022233428</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022232109</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022231654</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022231250</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022230955</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022230245</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022225723</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022224745</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022224153</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>1.568006</td>\n",
       "      <td>6.358174</td>\n",
       "      <td>3.286860</td>\n",
       "      <td>1.905812</td>\n",
       "      <td>8.779338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022223730</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>2.088780</td>\n",
       "      <td>1.780087</td>\n",
       "      <td>7.169304</td>\n",
       "      <td>3.348447</td>\n",
       "      <td>2.088780</td>\n",
       "      <td>9.446518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022223243</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.914568</td>\n",
       "      <td>1.646091</td>\n",
       "      <td>6.254367</td>\n",
       "      <td>3.025374</td>\n",
       "      <td>1.914568</td>\n",
       "      <td>8.770148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fuel-model</td>\n",
       "      <td>fuel-model-20221022193412-20221022221707</td>\n",
       "      <td>system.PipelineRun</td>\n",
       "      <td>FAILED</td>\n",
       "      <td>fuel-endpoint</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>main</td>\n",
       "      <td>{\"mae\": 10000}</td>\n",
       "      <td>us-central1</td>\n",
       "      <td>fuel-prediction</td>\n",
       "      <td>...</td>\n",
       "      <td>fuel_dataset</td>\n",
       "      <td>gabriela</td>\n",
       "      <td>n1-standard-4</td>\n",
       "      <td>vertex-ai-workshop-2022</td>\n",
       "      <td>1.750198</td>\n",
       "      <td>1.460680</td>\n",
       "      <td>5.680966</td>\n",
       "      <td>2.935248</td>\n",
       "      <td>1.750198</td>\n",
       "      <td>8.078310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment_name                                  run_name  \\\n",
       "0       fuel-model  fuel-model-20221023204859-20221023205033   \n",
       "1       fuel-model  fuel-model-20221022193412-20221023000459   \n",
       "2       fuel-model  fuel-model-20221022193412-20221022234350   \n",
       "3       fuel-model  fuel-model-20221022193412-20221022234031   \n",
       "4       fuel-model  fuel-model-20221022193412-20221022233707   \n",
       "5       fuel-model  fuel-model-20221022193412-20221022233428   \n",
       "6       fuel-model  fuel-model-20221022193412-20221022232109   \n",
       "7       fuel-model  fuel-model-20221022193412-20221022231654   \n",
       "8       fuel-model  fuel-model-20221022193412-20221022231250   \n",
       "9       fuel-model  fuel-model-20221022193412-20221022230955   \n",
       "10      fuel-model  fuel-model-20221022193412-20221022230245   \n",
       "11      fuel-model  fuel-model-20221022193412-20221022225723   \n",
       "12      fuel-model  fuel-model-20221022193412-20221022224745   \n",
       "13      fuel-model  fuel-model-20221022193412-20221022224153   \n",
       "14      fuel-model  fuel-model-20221022193412-20221022223730   \n",
       "15      fuel-model  fuel-model-20221022193412-20221022223243   \n",
       "16      fuel-model  fuel-model-20221022193412-20221022221707   \n",
       "\n",
       "              run_type     state param.endpoint_display_name  \\\n",
       "0   system.PipelineRun  COMPLETE               fuel-endpoint   \n",
       "1   system.PipelineRun  COMPLETE               fuel-endpoint   \n",
       "2   system.PipelineRun  COMPLETE               fuel-endpoint   \n",
       "3   system.PipelineRun    FAILED               fuel-endpoint   \n",
       "4   system.PipelineRun    FAILED               fuel-endpoint   \n",
       "5   system.PipelineRun    FAILED               fuel-endpoint   \n",
       "6   system.PipelineRun    FAILED               fuel-endpoint   \n",
       "7   system.PipelineRun    FAILED               fuel-endpoint   \n",
       "8   system.PipelineRun    FAILED               fuel-endpoint   \n",
       "9   system.PipelineRun    FAILED               fuel-endpoint   \n",
       "10  system.PipelineRun    FAILED               fuel-endpoint   \n",
       "11  system.PipelineRun    FAILED               fuel-endpoint   \n",
       "12  system.PipelineRun    FAILED               fuel-endpoint   \n",
       "13  system.PipelineRun    FAILED               fuel-endpoint   \n",
       "14  system.PipelineRun    FAILED               fuel-endpoint   \n",
       "15  system.PipelineRun    FAILED               fuel-endpoint   \n",
       "16  system.PipelineRun    FAILED               fuel-endpoint   \n",
       "\n",
       "   param.dataset_name param.table_name param.threshold_dict_str param.region  \\\n",
       "0        fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "1        fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "2        fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "3        fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "4        fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "5        fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "6        fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "7        fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "8        fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "9        fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "10       fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "11       fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "12       fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "13       fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "14       fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "15       fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "16       fuel_dataset             main           {\"mae\": 10000}  us-central1   \n",
       "\n",
       "   param.model_name  ... param.managed_dataset_name param.user  \\\n",
       "0   fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "1   fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "2   fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "3   fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "4   fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "5   fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "6   fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "7   fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "8   fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "9   fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "10  fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "11  fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "12  fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "13  fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "14  fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "15  fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "16  fuel-prediction  ...               fuel_dataset   gabriela   \n",
       "\n",
       "   param.endpoint_machine_type         param.project_id metric.test_loss  \\\n",
       "0                n1-standard-4  vertex-ai-workshop-2022         2.010309   \n",
       "1                n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "2                n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "3                n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "4                n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "5                n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "6                n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "7                n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "8                n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "9                n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "10               n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "11               n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "12               n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "13               n1-standard-4  vertex-ai-workshop-2022         1.905812   \n",
       "14               n1-standard-4  vertex-ai-workshop-2022         2.088780   \n",
       "15               n1-standard-4  vertex-ai-workshop-2022         1.914568   \n",
       "16               n1-standard-4  vertex-ai-workshop-2022         1.750198   \n",
       "\n",
       "   metric.loss metric.test_mse  metric.val_loss  metric.test_mae  \\\n",
       "0     1.679610        6.684991         3.377044         2.010309   \n",
       "1     1.568006        6.358174         3.286860         1.905812   \n",
       "2     1.568006        6.358174         3.286860         1.905812   \n",
       "3     1.568006        6.358174         3.286860         1.905812   \n",
       "4     1.568006        6.358174         3.286860         1.905812   \n",
       "5     1.568006        6.358174         3.286860         1.905812   \n",
       "6     1.568006        6.358174         3.286860         1.905812   \n",
       "7     1.568006        6.358174         3.286860         1.905812   \n",
       "8     1.568006        6.358174         3.286860         1.905812   \n",
       "9     1.568006        6.358174         3.286860         1.905812   \n",
       "10    1.568006        6.358174         3.286860         1.905812   \n",
       "11    1.568006        6.358174         3.286860         1.905812   \n",
       "12    1.568006        6.358174         3.286860         1.905812   \n",
       "13    1.568006        6.358174         3.286860         1.905812   \n",
       "14    1.780087        7.169304         3.348447         2.088780   \n",
       "15    1.646091        6.254367         3.025374         1.914568   \n",
       "16    1.460680        5.680966         2.935248         1.750198   \n",
       "\n",
       "    metric.test_mape  \n",
       "0           9.169966  \n",
       "1           8.779338  \n",
       "2           8.779338  \n",
       "3           8.779338  \n",
       "4           8.779338  \n",
       "5           8.779338  \n",
       "6           8.779338  \n",
       "7           8.779338  \n",
       "8           8.779338  \n",
       "9           8.779338  \n",
       "10          8.779338  \n",
       "11          8.779338  \n",
       "12          8.779338  \n",
       "13          8.779338  \n",
       "14          9.446518  \n",
       "15          8.770148  \n",
       "16          8.078310  \n",
       "\n",
       "[17 rows x 23 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_experiments_data(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5736899-a9fa-461d-801c-bbb8a1232b68",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fe796-61db-4780-9a53-6fef33f90596",
   "metadata": {},
   "source": [
    "## Online Predictions with Deployed Endpoint\n",
    "\n",
    "Retrieve the `Endpoint` deployed by the pipeline and use it to query your model for online predictions.\n",
    "\n",
    "Configure the `Endpoint()` function below with the following parameters:\n",
    "\n",
    "*  `endpoint_name`: A fully-qualified endpoint resource name or endpoint ID. Example: \"projects/123/locations/us-central1/endpoints/456\" or \"456\" when project and location are initialized or passed.\n",
    "*  `project_id`: GCP project.\n",
    "*  `location`: GCP region.\n",
    "\n",
    "Call `predict()` to return a prediction for a test review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "e1920256-da90-4813-bca1-99534e57c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID = 'insert-your-endpoint-id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "084cb500-73bd-4f3c-9e92-9dacc7e11bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = vertexai.Endpoint(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "fe2369f0-5962-4e58-b875-c4fcacc825ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = endpoint.predict([4,90.0,75.0,2125.0,14.5,74,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "a0ab35b6-171c-4577-bd59-62ce22636a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[26.7319756]], deployed_model_id='113350852730683392', model_version_id='1', model_resource_name='projects/525292812449/locations/us-central1/models/5413674197773713408', explanations=None)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0de80f-84f6-42fe-9504-a6aad0009924",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb3105-67a4-4c1e-bbc3-e932bc714c5c",
   "metadata": {},
   "source": [
    "## Batch Predictions with Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f857f07b-4f47-458b-b16e-7a44147772d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>8</td>\n",
       "      <td>351.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>3955.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>4</td>\n",
       "      <td>151.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2670.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>4</td>\n",
       "      <td>146.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3250.0</td>\n",
       "      <td>21.8</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2125.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1      2      3       4     5   6  7\n",
       "285  8  351.0  138.0  3955.0  13.2  79  1\n",
       "231  4   97.0   78.0  1940.0  14.5  77  2\n",
       "303  4  151.0   90.0  2670.0  16.0  79  1\n",
       "326  4  146.0   67.0  3250.0  21.8  80  2\n",
       "373  4   98.0   70.0  2125.0  17.3  82  1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create a fake batch file in Cloud Storage by randomly sampling our dataset\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(GCS_DATA_URI, header=None)\n",
    "# Remove label\n",
    "dataset = dataset.iloc[:,1:]\n",
    "\n",
    "batch_data = dataset.sample(10)\n",
    "batch_data.to_csv('data/batch_data_ex.csv', index=False)\n",
    "batch_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16c12614-d9cc-4e92-953a-a6c88f0eb0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: gs://vertex-ai-workshop-2022-ty8-bucket/data/batch_prediction/input_data/fuel_data_20221023204859.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://vertex-ai-workshop-2022-ty8-bucket/data/batch_prediction/input_data/fuel_data_20221023204859.csv'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Upload data to Cloud Storage\n",
    "from src.helper import upload_file_to_gcs\n",
    "\n",
    "gcs_batch_input_data_path = upload_file_to_gcs(\n",
    "    project_id=PROJECT_ID,\n",
    "    target=BUCKET_NAME,\n",
    "    source='data/batch_data_ex.csv',\n",
    "    blob_name=f'data/batch_prediction/input_data/fuel_data_{TIMESTAMP}.csv')\n",
    "gcs_batch_input_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f466b477-b0aa-4c70-807b-da6d3f6ea278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch job args\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\") \n",
    "batch_job_display_name = \"fuel-batch-prediction-job\"\n",
    "gcs_batch_data = gcs_batch_input_data_path\n",
    "instances_format = 'csv'\n",
    "gcs_dest_results = f'gs://{BUCKET_NAME}/batch_jobs/output/{TIMESTAMP}/'\n",
    "machine_type = \"n1-standard-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40fa8c45-ae83-411c-b360-e223c40f4e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "MODEL_ID             DISPLAY_NAME\n",
      "6220944430979874816  fuel-prediction\n",
      "5413674197773713408  fuel-prediction\n",
      "7656748287180931072  fuel-prediction\n",
      "148966233377603584   fuel-prediction\n",
      "9126047665610555392  fuel-prediction\n",
      "3195651381293744128  fuel-prediction\n",
      "5652364978024349696  fuel-prediction\n",
      "9015146524786556928  fuel-prediction\n",
      "3797726356477837312  fuel-prediction\n",
      "2407802921480617984  fuel-prediction\n"
     ]
    }
   ],
   "source": [
    "## List all Models and pick the Model ID \n",
    "!gcloud ai models list --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "097fc5c8-cd36-4ec1-8535-e9dca9a710b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'insert-your-model-id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ece47377-f591-49e1-a0e8-bc42e2f9f4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/vertex-ai-workshop-2022/locations/us-central1/models/6220944430979874816'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_resource_name = f'projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}'\n",
    "model_resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec760737-e25c-4aad-bf6e-01e31f3a4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiplatform.init(project=project, location=location)\n",
    "model = aip.Model(model_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84f440-2616-49ca-8327-3549fe286190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating BatchPredictionJob\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob created. Resource name: projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob created. Resource name: projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use this BatchPredictionJob in another session:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:To use this BatchPredictionJob in another session:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpj = aiplatform.BatchPredictionJob('projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:bpj = aiplatform.BatchPredictionJob('projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/5506229712210362368?project=525292812449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/5506229712210362368?project=525292812449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:BatchPredictionJob projects/525292812449/locations/us-central1/batchPredictionJobs/5506229712210362368 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=batch_job_display_name,\n",
    "        instances_format='csv', #json\n",
    "        gcs_source=[gcs_batch_data],\n",
    "        gcs_destination_prefix=gcs_dest_results,\n",
    "        machine_type=machine_type, # must be present      \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123dccc3-0ff0-461b-88b3-9aa5fa1de694",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81c780-5857-4837-9668-a3682daa3975",
   "metadata": {},
   "source": [
    "# IMPORTANT! CLEAN UP ALL RESOURCES CREATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15cb4a-a74b-4985-bfab-8be3dbabcec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
