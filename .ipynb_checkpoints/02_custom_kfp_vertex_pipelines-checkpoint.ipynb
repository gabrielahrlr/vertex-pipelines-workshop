{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8184cc-3583-4187-b58b-02c48d08ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57301cb-c630-41ab-90a0-30442ea38e06",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 02 - Vertex Pipelines using Custom KubeFlow Components\n",
    "\n",
    "## Overview \n",
    "\n",
    "This notebook shows how to use Kubeflow components to build a custom regression workflow on `Vertex AI Pipelines`.\n",
    "\n",
    "You will build a pipeline in this notebook that looks like this:\n",
    "\n",
    "<img src=\"img/vertex-pipelines.png\" width=\"80%\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Notebook Objective\n",
    "\n",
    "In this notebook, you will learn to use `Vertex AI Pipelines`, with only  `KubeFlow Components` to build a `custom` tabular regression model. In the pipeline we  will orchestrate data creation, data processing, model training and evaluation, and model deployment. We'll also see how to send payloads the endpoint deployed and how to run batch predition jobs.\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- `BigQuery`\n",
    "- `Vertex AI Pipelines`\n",
    "- `Google Cloud Pipeline Components`\n",
    "- `Vertex AI Model`\n",
    "- `Vertex AI Model Registry`\n",
    "- `Vertex AI Metadata`\n",
    "- `Vertex AI Endpoint`\n",
    "\n",
    "The steps performed in this notebook include:\n",
    "\n",
    "1. [Load Configuration settings from the setup notebook](#Load-Configuration-settings-from-the-setup-notebook)\n",
    "1. [Vertex Pipelines Introduction](#Vertex-Pipelines-Introduction)\n",
    "1. [Create a KFP Pipeline](#Create-a-KFP-Pipeline)\n",
    "    1. [Create a dataset in BigQuery](#Step-1:-Create-a-dataset-in-BigQuery)\n",
    "    1. [Transform the Data](#Step-2:-Transform-the-Data)\n",
    "    1. [Train and Evaluate our custom Regression Model](#Step-3:-Train-and-Evaluate-our-custom-Regression-Model)\n",
    "    1. [Upload Model to Vertex AI and Deploy Endpoint](#Step-4:-Upload-Model-to-Vertex-AI-and-Deploy-Endpoint)\n",
    "1. [Compile the KFP Pipeline](#Compile-the-KFP-Pipeline)\n",
    "1. [Execute the KFP Pipeline using Vertex AI Pipelines](#Execute-the-KFP-Pipeline-using-Vertex-AI-Pipelines)\n",
    "1. [Inspect Experiments](#Inspect-Experiments)\n",
    "1. [Online Predictions with Deployed Endpoint](#Online-Predictions-with-Deployed-Endpoint)\n",
    "1. [Batch Predictions with Created Model](#Batch-Predictions-with-Created-Model)\n",
    "\n",
    "\n",
    "The KubeFlow Compoenents are [documented here](https://www.kubeflow.org/docs/components/pipelines/v1/sdk-v2/python-function-components/)\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this workshop we'll use the **public datase**t [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) for demonstration purposes. The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes. The objective will be to build a model to predict \"MPG\" (Miles per Gallon).\n",
    "\n",
    "Check notebook  `01_exploratory_data_analysis.ipynb` for further details of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f8fe7-3013-4671-b5ad-fc5931a6251f",
   "metadata": {},
   "source": [
    "## Load Configuration settings from the setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed77f6d-ebae-413d-b2dc-ddd0cd85252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our configurations from notebook 00_environment_setup.ipynb\n",
    "from src.config import config\n",
    "\n",
    "PROJECT_ID = config['PROJECT_ID']\n",
    "REGION = config['REGION']\n",
    "ID = config['ID']\n",
    "BUCKET_NAME = config['BUCKET_NAME']\n",
    "GCS_DATA_URI = config['GCS_DATA_URI']\n",
    "BQ_DATASET_URI = config['BQ_DATASET_URI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a110447-b83e-42c2-923d-36ea9d314329",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719f7ec-8eaf-4892-9eef-b920c3574ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Import the Vertex AI Python SDK \n",
    "from google.cloud import aiplatform as aip\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import google.auth\n",
    "from google.cloud import storage\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "# kfp sdk, to create the Vertex AI Pipelines\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, Model, Output, Metrics, ClassificationMetrics, component, OutputPath, InputPath)\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "\n",
    "# TensorFlow model building libraries.\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Custom Modules\n",
    "from src.helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb9ee9-013f-492d-99a5-2244a62eb43d",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92536907-36a6-4a7c-a96f-f5f589357180",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b959a51-bc11-44aa-86fe-fe2a2048f8d7",
   "metadata": {},
   "source": [
    "## Vertex Pipelines Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27cd415-58cf-41a1-8b62-94827d6495bd",
   "metadata": {},
   "source": [
    "[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner, and storing your workflow's artifacts using Vertex ML Metadata. By storing the artifacts of your ML workflow in Vertex ML Metadata, you can analyze the lineage of your workflow's artifacts â€” for example, an ML model's lineage may include the training data, hyperparameters, and code that were used to create the model.\n",
    "\n",
    "You can build your Pipelines using the battle-tested and easy-to-use `KubeFlow Pipelines (KFP) SDK` or ` TensorFlow Extended (TFX) SDK`. \n",
    "\n",
    "Within your Vertex Pipeline with `KubeFlow Pipelines (KFP) SDK` you can use either your own custom components using `KubeFlow Components` or already-built compontents using `Google Cloud Pipeline Components`.\n",
    "\n",
    "The Google Cloud Components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform). \n",
    "\n",
    "The KubeFlow Compoenents are [documented here](https://www.kubeflow.org/docs/components/pipelines/v1/sdk-v2/python-function-components/)\n",
    "\n",
    "<img src=img/vertex-pipelines-def.png width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a7e96-de39-4511-b51c-b2e84e9e7dfc",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48781f94-263c-49dd-b7f2-261b65084201",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561d229-ec9e-4e05-9086-6bdd533a009a",
   "metadata": {},
   "source": [
    "## Create a KFP Pipeline\n",
    "\n",
    "To address your business requirements and get your higher performing model into production to deliver value faster, you will define a pipeline using the [**Kubeflow Pipelines (KFP) V2 SDK**](https://www.kubeflow.org/docs/components/pipelines/sdk/v2/v2-compatibility) to orchestrate the training and deployment of your model on [**Vertex Pipelines**](https://cloud.google.com/vertex-ai/docs/pipelines) below.\n",
    "\n",
    "The pipeline consists of four custom KFP V2 components:\n",
    "\n",
    "* `create_bq_dataset_op`: Create BigQuery Dataset from data stored as csv in Google Cloud Storage.\n",
    "\n",
    "* `prepare_datasets_op`: gets the required data and transform it for the training requirements.\n",
    "\n",
    "* `train_evaluate_model_op`: Trains our regression model to predig MPG and  evaluates the trained model and makes the decision whether to deploy the model and create an endpoint based on a passed threshold.\n",
    "\n",
    "\n",
    "*  `deploy_model_endpoint Custom Component`: Uploads trained model to Vertex AI and creates a Google Cloud Vertex Endpoint resource that maps physical machine resources with your model to enable it to serve online predictions. Online predictions have low latency requirements; providing resources to the model in advance reduces latency. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90b514-a374-44f7-8327-dc41e74e8732",
   "metadata": {},
   "source": [
    "### Step 1: Create a dataset in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868266df-885f-4095-b8b7-e518fb6f92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['google-cloud-bigquery', 'pandas'],\n",
    "    output_component_file=\"create_bq_dataset_op.yml\"\n",
    ")\n",
    "def create_bq_dataset_op(\n",
    "    gcs_uri: str,\n",
    "    project_id: str,\n",
    "    dataset_name: str,\n",
    "    table_name: str,\n",
    ") -> NamedTuple('Outputs',[('bq_dataset_uri', str)]):\n",
    "    \"\"\"\n",
    "    Create a new bucket in the US region with the STANDARD storage class\n",
    "    Args:\n",
    "        gcs_uri: gcs uri (gs://...)\n",
    "        bucket_name: name of the bucket\n",
    "        region: region or zone\n",
    "        service_account: service account\n",
    "    Output:\n",
    "        table_id:string, Table in BigQuery\n",
    "    \"\"\"\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    import os\n",
    "\n",
    "    # Create bigquery table\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    dataset_name = dataset_name\n",
    "\n",
    "    dataset_id = \"{}.{}\".format(bq_client.project, dataset_name)\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    try:\n",
    "        dataset = bq_client.create_dataset(dataset, timeout=30)\n",
    "        print(\"Created dataset {}.{}\".format(bq_client.project, dataset.dataset_id))\n",
    "    except:\n",
    "        bq_client.delete_dataset(dataset_id, delete_contents=True)\n",
    "        dataset = bq_client.create_dataset(dataset, timeout=30)\n",
    "\n",
    "    # Create table\n",
    "    table_name = table_name\n",
    "    table_id = f\"{dataset_id}.{table_name}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"mpg\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"cyl\", bigquery.enums.SqlTypeNames.INTEGER),\n",
    "            bigquery.SchemaField(\"dis\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"hp\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"weight\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"accel\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"year\", bigquery.enums.SqlTypeNames.INTEGER),\n",
    "            bigquery.SchemaField(\"origin\", bigquery.enums.SqlTypeNames.INTEGER),\n",
    "        ], \n",
    "        write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "    job = bq_client.load_table_from_uri(\n",
    "        gcs_uri, table_id, job_config=job_config)\n",
    "\n",
    "    job.result()\n",
    "\n",
    "    bq_dataset_uri = f\"bq://{dataset_id}.{table_name}\"\n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('Outputs',['bq_dataset_uri'])\n",
    "\n",
    "    return output(bq_dataset_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99265ece-aa4f-4b95-a4d0-21a2fffefe70",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a9238-094d-45aa-849f-76170342ef45",
   "metadata": {},
   "source": [
    "### Step 2: Transform the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d84833-fb68-495a-a814-8de308401b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install= ['google-cloud-bigquery[bqstorage,pandas]'],\n",
    "    output_component_file=\"prepare_datasets_op.yml\"\n",
    ")\n",
    "def prepare_datasets_op(\n",
    "    bq_dataset: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    ")-> NamedTuple(\"Outputs\", [(\"train_dataset_path\", str), (\"test_dataset_path\", str)]):\n",
    "    \n",
    "    # print(f\"Input dataset is: {bq_dataset}\")\n",
    "    # print(f\"Input dataset is: {bq_dataset.uri}\")\n",
    "    # print(f\"Input dataset is: {bq_dataset.location}\")\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import os\n",
    "    \n",
    "    \n",
    "    \n",
    "    bqclient = bigquery.Client()\n",
    "    \n",
    "    def download_table(bq_table_uri: str):\n",
    "        prefix = \"bq://\"\n",
    "        if bq_table_uri.startswith(prefix):\n",
    "            bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "        table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "        rows = bqclient.list_rows(\n",
    "            table,\n",
    "        )\n",
    "        return rows.to_dataframe(create_bqstorage_client=False)\n",
    "    \n",
    "    \n",
    "    raw_dataset = download_table(bq_dataset)\n",
    "    raw_dataset.rename(columns = {\n",
    "        'mpg':'MPG',\n",
    "        'cyl':'Cylinders',\n",
    "        'dis':'Displacement',\n",
    "        'hp': 'Horsepower',\n",
    "        'weight': 'Weight',\n",
    "        'accel': 'Acceleration',\n",
    "        'year': 'Model Year',\n",
    "        'origin': 'Origin'}, inplace = True)\n",
    "\n",
    "    # Get data in shape\n",
    "    dataset = raw_dataset.copy()\n",
    "    dataset.tail()\n",
    "    dataset = dataset.dropna()\n",
    "    dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "    dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\n",
    "    train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    \n",
    "    train_dataset_path = dataset_train.path\n",
    "    test_dataset_path = dataset_test.path\n",
    "    logging.info(f\"Dataset Train is be stored in: {dataset_train.path}\")\n",
    "    logging.info(f\"Dataset Test is be stored in: {dataset_test.path}\")\n",
    "    \n",
    "    train_dataset.to_csv(dataset_train.path, index=False)\n",
    "    test_dataset.to_csv(dataset_train.path, index=False)\n",
    "    \n",
    "    return (train_dataset_path, test_dataset_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a56bb-c851-481c-9b15-32e9897a1519",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09e711-4b50-425c-999d-d81888a6eaf2",
   "metadata": {},
   "source": [
    "### Step 3: Train and Evaluate our custom Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ccea5-493d-4e62-b1f0-89840a88f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"tensorflow/tensorflow:2.10.0\",\n",
    "    packages_to_install=['pandas'],\n",
    "    output_component_file=\"train_evaluate_model_op.yml\"\n",
    ")\n",
    "def train_evaluate_model_op(\n",
    "    dataset_train: Input[Dataset],\n",
    "    dataset_test: Input[Dataset],\n",
    "    threshold_dict_str: str,\n",
    "    pipeline_name: str,\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model],\n",
    "    model_uri: OutputPath(str),\n",
    ") -> NamedTuple(\n",
    "        'Outputs',[\n",
    "            ('loss', float),\n",
    "            ('val_loss', float),\n",
    "            ('dep_decision', str),\n",
    "            ('model_path', str)\n",
    "        ]):\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import tensorflow as tf\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    import json\n",
    "    \n",
    "    logging.info(f\"Train Dataset path {dataset_train.path}\")\n",
    "    logging.info(f\"Test Dataset path {dataset_test.path}\")\n",
    "    logging.info(f\"train dataset - {dataset_train.path}\")\n",
    "    \n",
    "    train_dataset = pd.read_csv(dataset_train.path)\n",
    "    test_dataset = pd.read_csv(dataset_train.path)\n",
    "    train_features = train_dataset.copy()\n",
    "    test_features = test_dataset.copy()\n",
    "    train_labels = train_features.pop('MPG')\n",
    "    test_labels = test_features.pop('MPG')\n",
    "\n",
    "    \n",
    "    # Create model\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    first = np.array(train_features[:1])\n",
    "    horsepower = np.array(train_features['Horsepower'])\n",
    "    horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
    "    horsepower_normalizer.adapt(horsepower)\n",
    "\n",
    "    def build_and_compile_model(norm):\n",
    "        model = keras.Sequential([\n",
    "            norm,\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        metrics_tf = [tf.metrics.MeanAbsoluteError(), tf.metrics.MeanAbsolutePercentageError(), \n",
    "               tf.metrics.MeanSquaredError()]\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error', \n",
    "                      optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                      metrics=metrics_tf\n",
    "                     )\n",
    "        return model\n",
    "\n",
    "    dnn_model = build_and_compile_model(normalizer)\n",
    "    dnn_model.summary()\n",
    "\n",
    "    history = dnn_model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        validation_split=0.2,\n",
    "        verbose=0, epochs=100\n",
    "    )\n",
    "\n",
    "    \n",
    "    loss_test, mae, mape, mse = dnn_model.evaluate(test_features,\n",
    "                                                       test_labels,\n",
    "                                                       verbose=0\n",
    "                                                      )\n",
    "    \n",
    "    logging.info(f\"TEST METRICS: {loss_test}, {mae}, {mape}, {mse}\")\n",
    "\n",
    "    # Log metrics\n",
    "    metrics_training = {metric: values[-1] for metric, values in history.history.items()}\n",
    "    metrics.log_metric('loss', metrics_training['loss'])\n",
    "    metrics.log_metric('val_loss', metrics_training['val_loss'])\n",
    "    metrics.log_metric('test_loss', loss_test)\n",
    "    metrics.log_metric('test_mae', mae)\n",
    "    metrics.log_metric('test_mape', mape)\n",
    "    metrics.log_metric('test_mse', mse)\n",
    "  \n",
    "    \n",
    "    model.metadata['loss'] = metrics_training['loss']\n",
    "    model.metadata['val_loss'] = metrics_training['val_loss']\n",
    "    model.metadata['test_loss'] = loss_test\n",
    "    model.metadata['pipeline'] = pipeline_name\n",
    "\n",
    "    # Save the model to GCS\n",
    "    dnn_model.save(model.path)\n",
    "\n",
    "    threshold_dict = json.loads(threshold_dict_str)\n",
    "    kpi_decision = float(threshold_dict['mae'])\n",
    "    \n",
    "        \n",
    "    if mae <= kpi_decision:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(f\"deployment decision is {dep_decision}\")\n",
    "    logging.info(f\"model will be stored in {model.path}\")\n",
    "    \n",
    "    model_uri = model.path\n",
    "\n",
    "    return (metrics_training['loss'], metrics_training['val_loss'], dep_decision, model.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee292b-1eef-4934-b6f3-92796a1a8ca2",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db9ace-a640-476f-87c0-dbae1a2989d7",
   "metadata": {},
   "source": [
    "### Step 4: Upload Model to Vertex AI and Deploy Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9dca3-3339-4d53-9b89-0d0301f18eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  base_image=\"python:3.9\",\n",
    "  packages_to_install=['google-cloud-aiplatform'],\n",
    "  output_component_file=\"upload_model_op.yml\"\n",
    ")\n",
    "def deploy_model_endpoint_op(\n",
    "    model: Input[Model],\n",
    "    region: str,\n",
    "    model_name: str,\n",
    "    pipeline_name: str,\n",
    "    project_id: str,\n",
    "    serving_container_image_uri: str,\n",
    "    machine_type: str,\n",
    "    model_display_name: str,\n",
    "    endpoint_display_name:str,\n",
    "    vertex_model: Output[Artifact]\n",
    "):\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as aip\n",
    "        \n",
    "    labels={'pipeline': str(pipeline_name),\n",
    "            'country': 'germany'}\n",
    "   \n",
    "    aip.init(project=project_id, location=region)\n",
    "\n",
    "    logging.info(f\"Model URI {model.uri}\")\n",
    "    logging.info(f\"Model Name: {model_name}\")\n",
    "    logging.info(f\"Model Labels: {labels}\")\n",
    "    logging.info(f\"serving_container_image_uri {serving_container_image_uri}\")\n",
    "    \n",
    "    def create_endpoint(endpoint_display_name, project, region):\n",
    "        endpoints = aip.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(endpoint_display_name),\n",
    "        order_by='create_time desc',\n",
    "        project=project, \n",
    "        location=region,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0]  # most recently created\n",
    "        else:\n",
    "            endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=endpoint_display_name, project=project, location=region\n",
    "        )\n",
    "        return endpoint\n",
    "    \n",
    "    # Create Endpoint     \n",
    "    endpoint = create_endpoint(endpoint_display_name, project_id, region) \n",
    "    \n",
    "    \n",
    "    model_upload = aip.Model.upload(  \n",
    "        display_name=model_name,\n",
    "        artifact_uri=model.uri,\n",
    "        description='Regression model for fuel prediction',\n",
    "        labels=labels,\n",
    "        serving_container_image_uri=serving_container_image_uri\n",
    "        )\n",
    "    \n",
    "    logging.info(f'Input Endpoint {endpoint}')\n",
    "    model_deploy = model_upload.deploy(\n",
    "        machine_type=machine_type, \n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=model_display_name,\n",
    "    )\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = model_deploy.resource_name\n",
    "    logging.info(model_deploy.resource_name)\n",
    "    \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd4b7c-1b4f-47f3-9105-b638651ecf12",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a73721-eee4-4f0c-a772-76583d451090",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile the KFP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e884bf3-5834-4374-9695-35a44479f947",
   "metadata": {},
   "source": [
    "#### Define Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bb60e-dac5-4e6b-a72f-86d807717463",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "USER = 'add-your-name-lowercase'\n",
    "USER = 'gabriela'\n",
    "if USER == 'add-your-name-lowercase':\n",
    "    USER = 'unknown'\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = \"fuel-model\"\n",
    "EXPERIMENT_DESCRIPTION = \"Fuel prediction pipeline\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/{USER}\"\n",
    "\n",
    "## Important the pipeline name always has the timestamp as part of the name.\n",
    "PIPELINE_NAME = f\"{EXPERIMENT_NAME}-{TIMESTAMP}\"\n",
    "PIPELINE_PACKAGE_PATH = f'{PIPELINE_NAME}-path.json'\n",
    "\n",
    "\n",
    "MODEL_NAME = EXPERIMENT_NAME\n",
    "THRESHOLD_DICT_STR = '{\"mae\": 10000}'\n",
    "\n",
    "DATASET_NAME = 'fuel_dataset'\n",
    "TABLE_NAME = 'main'\n",
    "MODEL_NAME = 'fuel-prediction'\n",
    "DATA_GCS_DIR = GCS_DATA_URI\n",
    "\n",
    "\n",
    "ENDPOINT_DISPLAY_NAME = 'fuel-endpoint'\n",
    "SERVING_CONTAINER_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'\n",
    "ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "\n",
    "LABELS = {\n",
    "    'creator': USER,\n",
    "    'workflow': 'fuel-prediction',\n",
    "    'type': 'regression'}\n",
    "\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_NAME,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    experiment_description=\"Fuel prediction pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8691e67-66c8-4021-80b4-af004b4619bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=PIPELINE_NAME\n",
    ")\n",
    "def pipeline(\n",
    "    gcs_uri: str,\n",
    "    dataset_name: str,\n",
    "    threshold_dict_str: str,\n",
    "    user:str,\n",
    "    managed_dataset_name: str,\n",
    "    endpoint_display_name: str,\n",
    "    endpoint_machine_type:str,\n",
    "    table_name: str,\n",
    "    project_id: str,\n",
    "    pipeline_name: str,\n",
    "    serving_container_image_uri:str,\n",
    "    model_name: str,\n",
    "    region: str):\n",
    "\n",
    "    # STEP 1:  Create bq table and dataset\n",
    "    create_bq_dataset_task = create_bq_dataset_op(\n",
    "        gcs_uri=gcs_uri,\n",
    "        project_id=project_id,\n",
    "        dataset_name=dataset_name,\n",
    "        table_name=table_name\n",
    "        ).set_caching_options(True) \\\n",
    "        .set_display_name('create-bq-dataset-table-op')\n",
    "    \n",
    "    \n",
    "    \n",
    "   # STEP 2: Prepare train and test datasets\n",
    "    prepare_data_op = prepare_datasets_op(\n",
    "        #bq_dataset=create_dataset_op.outputs['dataset']\n",
    "        bq_dataset =create_bq_dataset_task.outputs['bq_dataset_uri']\n",
    "    ).set_caching_options(True) \\\n",
    "        .set_display_name('prepare-datasets-op')\n",
    "\n",
    "    # STEP 3: Train and evaluate model\n",
    "    train_evaluate_model_task = train_evaluate_model_op(\n",
    "        dataset_train = prepare_data_op.outputs['dataset_train'],\n",
    "        dataset_test = prepare_data_op.outputs['dataset_test'],\n",
    "        threshold_dict_str=threshold_dict_str,\n",
    "        pipeline_name=pipeline_name\n",
    "    ).set_display_name('training-evaluation-job-op')\\\n",
    "        .set_caching_options(True)\n",
    "\n",
    "    ## Step3: Decision: If model performs according to our threshold, then deploy model and Enp\n",
    "    with dsl.Condition(\n",
    "            train_evaluate_model_task.outputs[\"dep_decision\"] == \"true\",\n",
    "            name=\"deploy_decision\",\n",
    "        ):\n",
    "            # Upload Model, Create a Vertex Endpoint resource and Deploy Model to Endpoint.\n",
    "\n",
    "            deploy_model_endpoint_op(\n",
    "                            model=train_evaluate_model_task.outputs['model'],\n",
    "                            region=region,\n",
    "                endpoint_display_name=endpoint_display_name,\n",
    "                            model_name=model_name,\n",
    "                            pipeline_name=pipeline_name,\n",
    "                            project_id=project_id,\n",
    "                machine_type=endpoint_machine_type,\n",
    "                model_display_name=model_name,\n",
    "                serving_container_image_uri=serving_container_image_uri\n",
    "            ).set_display_name('deploy-model-op').set_caching_options(True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6939c-1fa4-405f-8fbf-527e36966561",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ec150-96ef-4f89-8507-4378c58e1f19",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997c21e-2436-4262-b606-cf693d2c1f44",
   "metadata": {},
   "source": [
    "### Execute the KFP Pipeline using Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27876437-0828-44d2-b608-3751051fd2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=True,\n",
    "    labels=LABELS,\n",
    "    parameter_values={\n",
    "        'gcs_uri': DATA_GCS_DIR,\n",
    "        'dataset_name': DATASET_NAME,\n",
    "        'threshold_dict_str':THRESHOLD_DICT_STR,\n",
    "        'user': USER,\n",
    "        'managed_dataset_name': DATASET_NAME,\n",
    "        'endpoint_display_name': ENDPOINT_DISPLAY_NAME,\n",
    "        'endpoint_machine_type': ENDPOINT_MACHINE_TYPE,\n",
    "        'table_name': TABLE_NAME,\n",
    "        'project_id': PROJECT_ID,\n",
    "        'pipeline_name': PIPELINE_NAME,\n",
    "        'serving_container_image_uri': SERVING_CONTAINER_IMAGE_URI,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'region': REGION\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "job.submit(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b36214-0ad4-4459-85f9-917e957404ca",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57f6e2-a4d8-40e4-bdc3-51f1f47a1a43",
   "metadata": {},
   "source": [
    "## Inspect Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bf9aa-d771-497b-acce-5e2f7efa5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiments_data(\n",
    "  experiment_name: str,\n",
    "  project: str,\n",
    "  location: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Get experiments\n",
    "    \"\"\"\n",
    "    aip.init(experiment=experiment_name, project=project, location=location)\n",
    "    experiments_df = aip.get_experiment_df()\n",
    "    return experiments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcf265-bd3f-41f3-a8fd-67a800c0730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_experiments_data(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5736899-a9fa-461d-801c-bbb8a1232b68",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fe796-61db-4780-9a53-6fef33f90596",
   "metadata": {},
   "source": [
    "## Online Predictions with Deployed Endpoint\n",
    "\n",
    "Retrieve the `Endpoint` deployed by the pipeline and use it to query your model for online predictions.\n",
    "\n",
    "Configure the `Endpoint()` function below with the following parameters:\n",
    "\n",
    "*  `endpoint_name`: A fully-qualified endpoint resource name or endpoint ID. Example: \"projects/123/locations/us-central1/endpoints/456\" or \"456\" when project and location are initialized or passed.\n",
    "*  `project_id`: GCP project.\n",
    "*  `location`: GCP region.\n",
    "\n",
    "Call `predict()` to return a prediction for a test review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1920256-da90-4813-bca1-99534e57c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID = 'insert-your-endpoint-id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084cb500-73bd-4f3c-9e92-9dacc7e11bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = vertexai.Endpoint(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2369f0-5962-4e58-b875-c4fcacc825ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = endpoint.predict([4,90.0,75.0,2125.0,14.5,74,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab35b6-171c-4577-bd59-62ce22636a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0de80f-84f6-42fe-9504-a6aad0009924",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb3105-67a4-4c1e-bbc3-e932bc714c5c",
   "metadata": {},
   "source": [
    "## Batch Predictions with Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857f07b-4f47-458b-b16e-7a44147772d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a fake batch file in Cloud Storage by randomly sampling our dataset\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(GCS_DATA_URI, header=None)\n",
    "# Remove label\n",
    "dataset = dataset.iloc[:,1:]\n",
    "\n",
    "batch_data = dataset.sample(10)\n",
    "batch_data.to_csv('data/batch_data_ex.csv', index=False)\n",
    "batch_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c12614-d9cc-4e92-953a-a6c88f0eb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload data to Cloud Storage\n",
    "from src.helper import upload_file_to_gcs\n",
    "\n",
    "gcs_batch_input_data_path = upload_file_to_gcs(\n",
    "    project_id=PROJECT_ID,\n",
    "    target=BUCKET_NAME,\n",
    "    source='data/batch_data_ex.csv',\n",
    "    blob_name=f'data/batch_prediction/input_data/fuel_data_{TIMESTAMP}.csv')\n",
    "gcs_batch_input_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466b477-b0aa-4c70-807b-da6d3f6ea278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch job args\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\") \n",
    "batch_job_display_name = \"fuel-batch-prediction-job\"\n",
    "gcs_batch_data = gcs_batch_input_data_path\n",
    "instances_format = 'csv'\n",
    "gcs_dest_results = f'gs://{BUCKET_NAME}/batch_jobs/output/{TIMESTAMP}/'\n",
    "machine_type = \"n1-standard-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa8c45-ae83-411c-b360-e223c40f4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List all Models and pick the Model ID \n",
    "!gcloud ai models list --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fc5c8-cd36-4ec1-8535-e9dca9a710b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'insert-your-model-id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece47377-f591-49e1-a0e8-bc42e2f9f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resource_name = f'projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}'\n",
    "model_resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec760737-e25c-4aad-bf6e-01e31f3a4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiplatform.init(project=project, location=location)\n",
    "model = aip.Model(model_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84f440-2616-49ca-8327-3549fe286190",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=batch_job_display_name,\n",
    "        instances_format='csv', #json\n",
    "        gcs_source=[gcs_batch_data],\n",
    "        gcs_destination_prefix=gcs_dest_results,\n",
    "        machine_type=machine_type, # must be present      \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123dccc3-0ff0-461b-88b3-9aa5fa1de694",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81c780-5857-4837-9668-a3682daa3975",
   "metadata": {},
   "source": [
    "# IMPORTANT! CLEAN UP ALL RESOURCES CREATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15cb4a-a74b-4985-bfab-8be3dbabcec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
