{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da9da50-e440-441d-8dd7-9370ca75565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949f027-af7f-4c00-b834-6b887f8acf6a",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24876817-4b7c-4107-9cf7-fce3d023b9a9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, you learn to use `Vertex AI Pipelines`, `KubeFlow Components` and `Google Cloud Pipeline Components` to build a `custom` tabular regression model. `Vertex Pipelines workshop`, is a series of labs on how to build an end-to-end pipeline using Vertex Pipelines and Kubeflow Pipelines (kfp). In the pipeline we orchestrate data creation, data processing, model training and evaluation, and model deployment. We'll also see how to send payloads the endpoint deployed and how to run batch predition jobs. \n",
    "\n",
    "In this workshop we'll use the **public datase**t [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) for demonstration purposes. The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes. The objective will be to build a model to predict \"MPG\" (Miles per Gallon).\n",
    "\n",
    "The Google Cloud Components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform).\n",
    "\n",
    "The KubeFlow Compoenents are [documented here](https://www.kubeflow.org/docs/components/pipelines/v1/sdk-v2/python-function-components/)\n",
    "\n",
    "## Notebook Objective\n",
    "\n",
    "In this notebook, you will setup your environment to run the notebooks of this workshop.\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- `BigQuery`\n",
    "- `Vertex AI Pipelines`\n",
    "- `Google Cloud Pipeline Components`\n",
    "- `Vertex AI Model`\n",
    "- `Vertex AI Model Registry`\n",
    "- `Vertex AI Metadata`\n",
    "- `Vertex AI Endpoint`\n",
    "\n",
    "The steps performed in this notebook include:\n",
    "\n",
    "* [Setup your environment](#Setup-your-environment)\n",
    "* [Download Public Dataset Locally](#Download-Public-Dataset-Locally)\n",
    "* [Store Dataset in Google Cloud Storage](#Store-Dataset-in-Google-Cloud-Storage)\n",
    "* [Load Dataset into BigQuery](#Load-Dataset-into-BigQuery)\n",
    "* [Create config file](#Create-config-file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6687f33-6a17-4fca-b0cb-d6647df988f7",
   "metadata": {},
   "source": [
    "## Setup your environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cdfc6f-95f8-4f6a-b03c-a959ec492bd5",
   "metadata": {},
   "source": [
    "### Install additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d525ee-df89-4ed1-9fac-1bf9cf3266d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "!pip install --upgrade --no-warn-conflicts '{USER_FLAG}' -q \\\n",
    "    google-cloud-aiplatform \\\n",
    "    google-cloud-pipeline-components \\\n",
    "    facets-overview \\\n",
    "    ipywidgets \\\n",
    "    google-cloud-storage \\\n",
    "    tensorflow==2.8.0 \\\n",
    "    plotly==5.10.0 \\\n",
    "    itables==1.2.0 \\\n",
    "    plotly==5.10.0 \\\n",
    "    kfp==1.8.12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2efa165-0ca0-4021-8df4-bd2ab487aa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e53f82-b5c3-4bc1-821e-af195e800e09",
   "metadata": {},
   "source": [
    "### Configure Project Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be74818-55a6-40e3-b8d1-b8dc1b5d5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import random\n",
    "import string\n",
    "from typing import Union\n",
    "\n",
    "# Generate unique ID to help w/ unique naming of certain pieces\n",
    "ID = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=3))\n",
    "_ , PROJECT_ID = google.auth.default()\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-{ID}-bucket\"\n",
    "print('PROJECT_ID', PROJECT_ID)\n",
    "print('BUCKET_NAME', BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb8b4b-74c0-4abd-b804-1bd17b6c903e",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd31ca29-a32d-469c-9bb5-539251aebc9f",
   "metadata": {},
   "source": [
    "#### Create Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a07b6e-9655-492e-bac9-9b799835c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper import create_bucket\n",
    "\n",
    "# Create new bucket\n",
    "new_bucket_name, new_bucket_uri = create_bucket(\n",
    "  bucket_name=BUCKET_NAME,\n",
    "  region=REGION,\n",
    "  project_id=PROJECT_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdded0a-06bc-45f2-ad75-0c023978a45d",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21d328-f9f5-49ab-a1b6-5a4078c44b83",
   "metadata": {},
   "source": [
    "## Download Public Dataset Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf601d-f56c-45ef-ad79-856bbdc1f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c9380-f458-4b72-a188-0bc592343fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "\n",
    "column_names = [\n",
    "    'MPG', 'Cylinders', 'Displacement',\n",
    "    'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin'\n",
    "]\n",
    "\n",
    "raw_dataset = pd.read_csv(\n",
    "    url,\n",
    "    names=column_names,\n",
    "    na_values='?',\n",
    "    comment='\\t',\n",
    "    sep=' ',\n",
    "    skipinitialspace=True,\n",
    ")\n",
    "\n",
    "raw_dataset.rename(\n",
    "    columns = {\n",
    "        'MPG': 'mpg',\n",
    "        'Cylinders': 'cyl',\n",
    "        'Displacement': 'dis',\n",
    "        'Horsepower': 'hp',\n",
    "        'Weight': 'weight',\n",
    "        'Acceleration': 'accel',\n",
    "        'Model Year': 'year',\n",
    "        'Origin': 'origin'\n",
    "    }, inplace = True\n",
    ")\n",
    "\n",
    "raw_dataset.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c080ec2-fcb9-4753-9520-403f2d1b53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cfe776-81b1-46d3-b663-9ec238f0b194",
   "metadata": {},
   "source": [
    "#### Store dataset locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead4e86-ca47-4313-ac4a-0af21ed4b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = 'data/fuel_data.csv'\n",
    "raw_dataset.to_csv(header=False, index=False, path_or_buf=local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25708540-12b7-411e-a9fa-9a02f380163d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f157b0ed-9f49-441f-8b7c-28b23f4823b3",
   "metadata": {},
   "source": [
    "## Store Dataset in Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5034fa85-b490-4aad-89cd-229851fd7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper import upload_file_to_gcs\n",
    "\n",
    "gcs_data_path = upload_file_to_gcs(\n",
    "    project_id=PROJECT_ID,\n",
    "    target=BUCKET_NAME,\n",
    "    source=local_path,\n",
    "    blob_name='data/fuel_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c895ba4-ebe6-4dc6-9419-fbb392755f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507c753a-1b3e-4829-9f51-ddb1d6dd476b",
   "metadata": {},
   "source": [
    "## Load Dataset into BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2184eb6-3154-41df-b85d-d0c264e07a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bq_dataset(\n",
    "    gcs_uri: str,\n",
    "    project_id: str,\n",
    "    dataset_name: str,\n",
    "    table_name: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a new bucket in the US region with the STANDARD storage class\n",
    "    Args:\n",
    "        gcs_uri: gcs uri (gs://...)\n",
    "        bucket_name: name of the bucket\n",
    "        region: region or zone\n",
    "        service_account: service account\n",
    "    Output:\n",
    "        table_id:string, Table in BigQuery\n",
    "    \"\"\"\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    import os\n",
    "\n",
    "    # Create bigquery table\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    dataset_name = dataset_name\n",
    "\n",
    "    dataset_id = \"{}.{}\".format(bq_client.project, dataset_name)\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    try:\n",
    "        dataset = bq_client.create_dataset(dataset, timeout=30)\n",
    "        print(\"Created dataset {}.{}\".format(bq_client.project, dataset.dataset_id))\n",
    "    except:\n",
    "        bq_client.delete_dataset(dataset_id, delete_contents=True)\n",
    "        dataset = bq_client.create_dataset(dataset, timeout=30)\n",
    "\n",
    "    # Create table\n",
    "    table_name = table_name\n",
    "    table_id = f\"{dataset_id}.{table_name}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"mpg\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"cyl\", bigquery.enums.SqlTypeNames.INTEGER),\n",
    "            bigquery.SchemaField(\"dis\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"hp\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"weight\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"accel\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"year\", bigquery.enums.SqlTypeNames.INTEGER),\n",
    "            bigquery.SchemaField(\"origin\", bigquery.enums.SqlTypeNames.INTEGER),\n",
    "        ], \n",
    "        write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "    job = bq_client.load_table_from_uri(\n",
    "        gcs_uri, table_id, job_config=job_config)\n",
    "\n",
    "    job.result()\n",
    "\n",
    "    bq_dataset_uri = f\"bq://{dataset_id}.{table_name}\"\n",
    "    \n",
    "    return bq_dataset_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4968949a-d88e-424d-84e6-c4073392ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_dataset_uri = create_bq_dataset(\n",
    "    gcs_uri=gcs_data_path,\n",
    "    project_id=PROJECT_ID,\n",
    "    dataset_name='fuel_dataset',\n",
    "    table_name='main',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb095a1-d303-4d3e-8db1-0a95cce058ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_dataset_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72edd772-499a-453f-81b4-2e65b90700f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create config file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ba36e-78a7-48d0-80a1-9eb8c7a9bd54",
   "metadata": {},
   "source": [
    "#### Create Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f5d59-8c49-40d2-bbac-1d6b6f3841ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'PROJECT_ID':PROJECT_ID,\n",
    "    'REGION': REGION,\n",
    "    'ID': ID,\n",
    "    'BUCKET_NAME': BUCKET_NAME,\n",
    "    'GCS_DATA_URI': gcs_data_path,\n",
    "    'BQ_DATASET_URI': bq_dataset_uri\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d646b4-ecd2-4124-89c6-3c56a4218d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"src/config.py\", 'w') as f: \n",
    "    f.write(f\"config={json.dumps(config)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508cfbf-cea7-4523-a667-a420aa8d95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904058f-4394-4a45-b652-38a316ca3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8d57d-1af6-405d-90cd-58ec172fc9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['PROJECT_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0fc96-9d78-4be4-b488-4be93c6f36e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caaaf6a-cfa4-4dcf-968c-c1c6ffccc255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
