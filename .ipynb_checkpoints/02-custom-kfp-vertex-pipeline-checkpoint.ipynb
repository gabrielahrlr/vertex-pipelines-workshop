{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57301cb-c630-41ab-90a0-30442ea38e06",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vertex AI Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420c384-b5a8-4408-b29d-848e00420610",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Setup Environment](#I.Setup-Environment)\n",
    "    * [Install Libraries](#Install-Libraries)\n",
    "    * [Import Libraries](#Import-Libraries)\n",
    "    * [Define Constants and Variables](#Define-Constants-and-Variables)\n",
    "    * [Initialize Vertex AI](#Initialize-Vertex-AI)\n",
    "* [Download Workshop Data](#Download-Workshop-Data)\n",
    "* [Build our Vertex Pipeline](#II.-Vertex-Pipelines)\n",
    "    * [Create Pipeline Components](#Create-Pipeline-Components)\n",
    "* [Run & Explore Experiments](#Run-and-explore-experiments)\n",
    "* [Get Predictions](#Get-Predictions)\n",
    "    * [Get Online Predictions](#Get-Online-Predictions)\n",
    "    * [Get Batch Predictions](#Get-Batch-Predictions)\n",
    "* [Next Steps](#Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abaefc-663a-4186-9587-3dcc5126ab5a",
   "metadata": {},
   "source": [
    "<img src=img/custom-kfp-pipeline.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4cab4c-abe7-4a95-9de0-4d30554a8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b684947-f253-4de4-a376-982fd1e88e49",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca81cc1-c6a4-4b9b-80ce-4c66936b86de",
   "metadata": {},
   "source": [
    "In this workshop ... blablabla\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f8fe7-3013-4671-b5ad-fc5931a6251f",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb5edc-391b-4261-8466-63a7f09be5b5",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170ff26-acb9-4fbe-9d6b-2cfebbcab78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --user google-cloud-aiplatform==1.7.0 --upgrade -q --no-warn-conflicts\n",
    "# !pip3 install --user kfp==1.8.9 google-cloud-pipeline-components==1.0.7 -q --no-warn-conflicts\n",
    "# !pip3 install --user tensorflow -q --no-warn-conflicts\n",
    "# !pip install --user tf-models-official -q --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a110447-b83e-42c2-923d-36ea9d314329",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719f7ec-8eaf-4892-9eef-b920c3574ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Import the Vertex AI Python SDK \n",
    "from google.cloud import aiplatform as vertexai\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import google.auth\n",
    "import google.cloud.aiplatform as vertex\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "#from google.cloud import bigquery\n",
    "\n",
    "# google_cloud_pipeline_components includes pre-built KFP components for interfacing with Vertex AIservices.\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "\n",
    "# kfp sdk, to create the Vertex AI Pipelines\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline\n",
    "\n",
    "from typing import NamedTuple\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath,\n",
    "                        InputPath)\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "from typing import NamedTuple\n",
    "import google.cloud.aiplatform as aip\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component, Artifact, Dataset, Input, Metrics, Model, Output\n",
    "from google_cloud_pipeline_components.experimental import vertex_notification_email as gcc_exp\n",
    "\n",
    "# TensorFlow model building libraries.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Re-create the AdamW optimizer used in the original BERT paper.\n",
    "from official.nlp import optimization  \n",
    "\n",
    "# Libraries for data and plot model training metrics.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Own Functions\n",
    "from src.helper import upload_file_to_gcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21faf25-609d-4f76-88fd-58ecd8f2385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 show google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be3948-cca5-4a3f-92bd-cd2660a72157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import google.cloud.aiplatform as vertex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "\n",
    "_, PROJECT_ID = google.auth.default()\n",
    "REGION = \"us-central1\"\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "def get_service_account():\n",
    "    # Todo: use python client instead.\n",
    "    service_account = !gcloud iam service-accounts list --filter=\"Compute Engine default service account\"\n",
    "    service_account = np.array(service_account[1].split())[5]\n",
    "    SERVICE_ACCOUNT = f\"{service_account}\"\n",
    "    return SERVICE_ACCOUNT\n",
    "\n",
    "SERVICE_ACCOUNT = get_service_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca9e017-ccb1-4f93-b9f6-221df4198ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bucket\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-bucket\"\n",
    "\n",
    "# Check if Bucket already exists, if not, then create the bucket\n",
    "# Imports the Google Cloud client library\n",
    "\n",
    "# Instantiates a client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# Check if bucket already exists:\n",
    "buckets = [bucket.name for bucket in storage_client.list_buckets()]\n",
    "if not BUCKET_NAME in buckets:\n",
    "    bucket = storage_client.create_bucket(BUCKET_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdd9cac-8522-43c6-af19-230983540967",
   "metadata": {},
   "source": [
    "## Download Workshop Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54369b2-2f9e-424b-b341-0e05dde87cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "\n",
    "column_names = [\n",
    "    'MPG', 'Cylinders', 'Displacement',\n",
    "    'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin'\n",
    "]\n",
    "\n",
    "raw_dataset = pd.read_csv(\n",
    "    url,\n",
    "    names=column_names,\n",
    "    na_values='?',\n",
    "    comment='\\t',\n",
    "    sep=' ',\n",
    "    skipinitialspace=True,\n",
    ")\n",
    "\n",
    "raw_dataset.rename(\n",
    "    columns = {\n",
    "        'MPG': 'mpg',\n",
    "        'Cylinders': 'cyl',\n",
    "        'Displacement': 'dis',\n",
    "        'Horsepower': 'hp',\n",
    "        'Weight': 'weight',\n",
    "        'Acceleration': 'accel',\n",
    "        'Model Year': 'year',\n",
    "        'Origin': 'origin'\n",
    "    }, inplace = True\n",
    ")\n",
    "\n",
    "raw_dataset.dropna(inplace=True)\n",
    "gcs_path = 'data/fuel_data.csv'\n",
    "raw_dataset.to_csv(header=False, index=False, path_or_buf=gcs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78386f84-80a7-41b9-b98d-c2e56142408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_file_to_gcs(bucket_name=BUCKET_NAME, source_file_name='data/fuel_data.csv', destination_blob_name=gcs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb9ee9-013f-492d-99a5-2244a62eb43d",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92536907-36a6-4a7c-a96f-f5f589357180",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b959a51-bc11-44aa-86fe-fe2a2048f8d7",
   "metadata": {},
   "source": [
    "## Vertex Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27cd415-58cf-41a1-8b62-94827d6495bd",
   "metadata": {},
   "source": [
    "<img src=img/vertex-pipelines.png width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48781f94-263c-49dd-b7f2-261b65084201",
   "metadata": {},
   "source": [
    "To address your business requirements and get your higher performing model into production to deliver value faster, you will define a pipeline using the [**Kubeflow Pipelines (KFP) V2 SDK**](https://www.kubeflow.org/docs/components/pipelines/sdk/v2/v2-compatibility) to orchestrate the training and deployment of your model on [**Vertex Pipelines**](https://cloud.google.com/vertex-ai/docs/pipelines) below.\n",
    "\n",
    "The pipeline consists of five components:\n",
    "\n",
    "* `get_reviews_datasets Custom Component` [(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp): gets the required data and transform it for the training requirements.\n",
    "\n",
    "* `train_bert_classifier Custom Component` [(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp): trains your custom model using a custom component from kfp. This is the same as configuring a Vertex Custom Container Training Job using the Vertex Python SDK you covered in the Vertex AI: Qwik Start lab.\n",
    "\n",
    "* `classification_model_eval_metrics Custom Component` [(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp): evaluates the trained model and makes the decision whether to deploy the model and create an endpoint based on a passed threshold.\n",
    "\n",
    "\n",
    "*  `deploy_model_endpoint Custom Component` [(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.EndpointCreateOp): Creates a Google Cloud Vertex Endpoint resource that maps physical machine resources with your model to enable it to serve online predictions. Online predictions have low latency requirements; providing resources to the model in advance reduces latency. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561d229-ec9e-4e05-9086-6bdd533a009a",
   "metadata": {},
   "source": [
    "### Create Pipeline's Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90b514-a374-44f7-8327-dc41e74e8732",
   "metadata": {},
   "source": [
    "### Pipeline Step 1: Create BigQuery Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868266df-885f-4095-b8b7-e518fb6f92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=['google-cloud-bigquery', 'pandas'],\n",
    "    output_component_file=\"create_bq_dataset_op.yml\"\n",
    ")\n",
    "def create_bq_dataset_op(\n",
    "    gcs_uri: str,\n",
    "    project_id: str,\n",
    "    dataset_name: str,\n",
    "    table_name: str,\n",
    ") -> NamedTuple('Outputs',[('bq_dataset_uri', str)]):\n",
    "    \"\"\"\n",
    "    Create a new bucket in the US region with the STANDARD storage class\n",
    "    Args:\n",
    "        gcs_uri: gcs uri (gs://...)\n",
    "        bucket_name: name of the bucket\n",
    "        region: region or zone\n",
    "        service_account: service account\n",
    "    Output:\n",
    "        table_id:string, Table in BigQuery\n",
    "    \"\"\"\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    import os\n",
    "\n",
    "    # Create bigquery table\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    dataset_name = dataset_name\n",
    "\n",
    "    dataset_id = \"{}.{}\".format(bq_client.project, dataset_name)\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    try:\n",
    "        dataset = bq_client.create_dataset(dataset, timeout=30)\n",
    "        print(\"Created dataset {}.{}\".format(bq_client.project, dataset.dataset_id))\n",
    "    except:\n",
    "        bq_client.delete_dataset(dataset_id, delete_contents=True)\n",
    "        dataset = bq_client.create_dataset(dataset, timeout=30)\n",
    "\n",
    "    # Create table\n",
    "    table_name = table_name\n",
    "    table_id = f\"{dataset_id}.{table_name}\"\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField(\"mpg\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"cyl\", bigquery.enums.SqlTypeNames.INTEGER),\n",
    "            bigquery.SchemaField(\"dis\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"hp\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"weight\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"accel\", bigquery.enums.SqlTypeNames.FLOAT64),\n",
    "            bigquery.SchemaField(\"year\", bigquery.enums.SqlTypeNames.INTEGER),\n",
    "            bigquery.SchemaField(\"origin\", bigquery.enums.SqlTypeNames.INTEGER),\n",
    "        ], \n",
    "        write_disposition=\"WRITE_TRUNCATE\")\n",
    "\n",
    "    job = bq_client.load_table_from_uri(\n",
    "        gcs_uri, table_id, job_config=job_config)\n",
    "\n",
    "    job.result()\n",
    "\n",
    "    bq_dataset_uri = f\"bq://{dataset_id}.{table_name}\"\n",
    "    from collections import namedtuple\n",
    "    output = namedtuple('Outputs',['bq_dataset_uri'])\n",
    "\n",
    "    return output(bq_dataset_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a9238-094d-45aa-849f-76170342ef45",
   "metadata": {},
   "source": [
    "### Pipeline Step 2: Prepare train, val & test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d84833-fb68-495a-a814-8de308401b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install= ['google-cloud-bigquery[bqstorage,pandas]'],\n",
    "    output_component_file=\"prepare_datasets_op.yml\"\n",
    ")\n",
    "def prepare_datasets_op(\n",
    "    bq_dataset: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset],\n",
    ")-> NamedTuple(\"Outputs\", [(\"train_dataset_path\", str), (\"test_dataset_path\", str)]):\n",
    "    \n",
    "    # print(f\"Input dataset is: {bq_dataset}\")\n",
    "    # print(f\"Input dataset is: {bq_dataset.uri}\")\n",
    "    # print(f\"Input dataset is: {bq_dataset.location}\")\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    import os\n",
    "    \n",
    "    \n",
    "    \n",
    "    bqclient = bigquery.Client()\n",
    "    \n",
    "    def download_table(bq_table_uri: str):\n",
    "        prefix = \"bq://\"\n",
    "        if bq_table_uri.startswith(prefix):\n",
    "            bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "        table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "        rows = bqclient.list_rows(\n",
    "            table,\n",
    "        )\n",
    "        return rows.to_dataframe(create_bqstorage_client=False)\n",
    "    \n",
    "    \n",
    "    raw_dataset = download_table(bq_dataset)\n",
    "    raw_dataset.rename(columns = {\n",
    "        'mpg':'MPG',\n",
    "        'cyl':'Cylinders',\n",
    "        'dis':'Displacement',\n",
    "        'hp': 'Horsepower',\n",
    "        'weight': 'Weight',\n",
    "        'accel': 'Acceleration',\n",
    "        'year': 'Model Year',\n",
    "        'origin': 'Origin'}, inplace = True)\n",
    "\n",
    "    # Get data in shape\n",
    "    dataset = raw_dataset.copy()\n",
    "    dataset.tail()\n",
    "    dataset = dataset.dropna()\n",
    "    dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "    dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\n",
    "    train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "    test_dataset = dataset.drop(train_dataset.index)\n",
    "    \n",
    "    train_dataset_path = dataset_train.path\n",
    "    test_dataset_path = dataset_test.path\n",
    "    logging.info(f\"Dataset Train is be stored in: {dataset_train.path}\")\n",
    "    logging.info(f\"Dataset Test is be stored in: {dataset_test.path}\")\n",
    "    \n",
    "    train_dataset.to_csv(dataset_train.path, index=False)\n",
    "    test_dataset.to_csv(dataset_train.path, index=False)\n",
    "    \n",
    "    return (train_dataset_path, test_dataset_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac09e711-4b50-425c-999d-d81888a6eaf2",
   "metadata": {},
   "source": [
    "### Pipeline Step 3: Train & Evaluate our Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ccea5-493d-4e62-b1f0-89840a88f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"tensorflow/tensorflow:2.10.0\",\n",
    "    packages_to_install=['pandas'],\n",
    "    output_component_file=\"train_evaluate_model_op.yml\"\n",
    ")\n",
    "def train_evaluate_model_op(\n",
    "    dataset_train: Input[Dataset],\n",
    "    dataset_test: Input[Dataset],\n",
    "    threshold_dict_str: str,\n",
    "    pipeline_name: str,\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model],\n",
    "    model_uri: OutputPath(str),\n",
    ") -> NamedTuple(\n",
    "        'Outputs',[\n",
    "            ('loss', float),\n",
    "            ('val_loss', float),\n",
    "            ('dep_decision', str),\n",
    "            ('model_path', str)\n",
    "        ]):\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import tensorflow as tf\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    import json\n",
    "    \n",
    "    logging.info(f\"Train Dataset path {dataset_train.path}\")\n",
    "    logging.info(f\"Test Dataset path {dataset_test.path}\")\n",
    "    logging.info(f\"train dataset - {dataset_train.path}\")\n",
    "    \n",
    "    train_dataset = pd.read_csv(dataset_train.path)\n",
    "    test_dataset = pd.read_csv(dataset_train.path)\n",
    "    train_features = train_dataset.copy()\n",
    "    test_features = test_dataset.copy()\n",
    "    train_labels = train_features.pop('MPG')\n",
    "    test_labels = test_features.pop('MPG')\n",
    "\n",
    "    \n",
    "    # Create model\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    first = np.array(train_features[:1])\n",
    "    horsepower = np.array(train_features['Horsepower'])\n",
    "    horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
    "    horsepower_normalizer.adapt(horsepower)\n",
    "\n",
    "    def build_and_compile_model(norm):\n",
    "        model = keras.Sequential([\n",
    "            norm,\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        metrics_tf = [tf.metrics.MeanAbsoluteError(), tf.metrics.MeanAbsolutePercentageError(), \n",
    "               tf.metrics.MeanSquaredError()]\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error', \n",
    "                      optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "                      metrics=metrics_tf\n",
    "                     )\n",
    "        return model\n",
    "\n",
    "    dnn_model = build_and_compile_model(normalizer)\n",
    "    dnn_model.summary()\n",
    "\n",
    "    history = dnn_model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        validation_split=0.2,\n",
    "        verbose=0, epochs=100\n",
    "    )\n",
    "\n",
    "    \n",
    "    loss_test, mae, mape, mse = dnn_model.evaluate(test_features,\n",
    "                                                       test_labels,\n",
    "                                                       verbose=0\n",
    "                                                      )\n",
    "    \n",
    "    logging.info(f\"TEST METRICS: {loss_test}, {mae}, {mape}, {mse}\")\n",
    "\n",
    "    # Log metrics\n",
    "    metrics_training = {metric: values[-1] for metric, values in history.history.items()}\n",
    "    metrics.log_metric('loss', metrics_training['loss'])\n",
    "    metrics.log_metric('val_loss', metrics_training['val_loss'])\n",
    "    metrics.log_metric('test_loss', loss_test)\n",
    "    metrics.log_metric('test_mae', mae)\n",
    "    metrics.log_metric('test_mape', mape)\n",
    "    metrics.log_metric('test_mse', mse)\n",
    "  \n",
    "    \n",
    "    model.metadata['loss'] = metrics_training['loss']\n",
    "    model.metadata['val_loss'] = metrics_training['val_loss']\n",
    "    model.metadata['test_loss'] = loss_test\n",
    "    model.metadata['pipeline'] = pipeline_name\n",
    "\n",
    "    # Save the model to GCS\n",
    "    dnn_model.save(model.path)\n",
    "\n",
    "    threshold_dict = json.loads(threshold_dict_str)\n",
    "    kpi_decision = float(threshold_dict['mae'])\n",
    "    \n",
    "        \n",
    "    if mae <= kpi_decision:\n",
    "        dep_decision = \"true\"\n",
    "    else:\n",
    "        dep_decision = \"false\"\n",
    "    logging.info(f\"deployment decision is {dep_decision}\")\n",
    "    logging.info(f\"model will be stored in {model.path}\")\n",
    "    \n",
    "    model_uri = model.path\n",
    "\n",
    "    return (metrics_training['loss'], metrics_training['val_loss'], dep_decision, model.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db9ace-a640-476f-87c0-dbae1a2989d7",
   "metadata": {},
   "source": [
    "### Pipeline Step 4: Upload our Model to Model Registry, Create and Endpoint & Deploy our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9dca3-3339-4d53-9b89-0d0301f18eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  base_image=\"python:3.9\",\n",
    "  packages_to_install=['google-cloud-aiplatform'],\n",
    "  output_component_file=\"upload_model_op.yml\"\n",
    ")\n",
    "def deploy_model_endpoint_op(\n",
    "    model: Input[Model],\n",
    "    region: str,\n",
    "    model_name: str,\n",
    "    pipeline_name: str,\n",
    "    project_id: str,\n",
    "    serving_container_image_uri: str,\n",
    "    machine_type: str,\n",
    "    model_display_name: str,\n",
    "    endpoint_display_name:str,\n",
    "    vertex_model: Output[Artifact]\n",
    "):\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as aip\n",
    "        \n",
    "    labels={'pipeline': str(pipeline_name),\n",
    "            'country': 'germany'}\n",
    "   \n",
    "    aip.init(project=project_id, location=region)\n",
    "\n",
    "    logging.info(f\"Model URI {model.uri}\")\n",
    "    logging.info(f\"Model Name: {model_name}\")\n",
    "    logging.info(f\"Model Labels: {labels}\")\n",
    "    logging.info(f\"serving_container_image_uri {serving_container_image_uri}\")\n",
    "    \n",
    "    def create_endpoint(endpoint_display_name, project, region):\n",
    "        endpoints = aip.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(endpoint_display_name),\n",
    "        order_by='create_time desc',\n",
    "        project=project, \n",
    "        location=region,\n",
    "        )\n",
    "        if len(endpoints) > 0:\n",
    "            endpoint = endpoints[0]  # most recently created\n",
    "        else:\n",
    "            endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=endpoint_display_name, project=project, location=region\n",
    "        )\n",
    "        return endpoint\n",
    "    \n",
    "    # Create Endpoint     \n",
    "    endpoint = create_endpoint(endpoint_display_name, project_id, region) \n",
    "    \n",
    "    \n",
    "    model_upload = aip.Model.upload(  \n",
    "        display_name=model_name,\n",
    "        artifact_uri=model.uri,\n",
    "        description='Regression model for fuel prediction',\n",
    "        labels=labels,\n",
    "        serving_container_image_uri=serving_container_image_uri\n",
    "        )\n",
    "    \n",
    "    logging.info(f'Input Endpoint {endpoint}')\n",
    "    model_deploy = model_upload.deploy(\n",
    "        machine_type=machine_type, \n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        deployed_model_display_name=model_display_name,\n",
    "    )\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_model.uri = model_deploy.resource_name\n",
    "    logging.info(model_deploy.resource_name)\n",
    "    \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a73721-eee4-4f0c-a772-76583d451090",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build & Compile our Pipeline integrating all components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e884bf3-5834-4374-9695-35a44479f947",
   "metadata": {},
   "source": [
    "#### Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bb60e-dac5-4e6b-a72f-86d807717463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "USER = 'add-your-name-lowercase'\n",
    "if USER == 'add-your-name-lowercase':\n",
    "    USER = 'unknown'\n",
    "\n",
    "USER = 'gabriela'\n",
    "\n",
    "EXPERIMENT_NAME = \"fuel-model\"\n",
    "EXPERIMENT_DESCRIPTION = \"Fuel prediction pipeline\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/{USER}\"\n",
    "PIPELINE_NAME = f\"{EXPERIMENT_NAME}-{TIMESTAMP}\"\n",
    "PIPELINE_PACKAGE_PATH = f'{PIPELINE_NAME}-path.json'\n",
    "\n",
    "\n",
    "MODEL_NAME = EXPERIMENT_NAME\n",
    "THRESHOLD_DICT_STR = '{\"mae\": 10000}'\n",
    "\n",
    "MANAGED_DATASET_NAME = 'fuel_dataset'\n",
    "TABLE_NAME = 'main'\n",
    "MODEL_NAME = 'fuel-prediction'\n",
    "DATA_GCS_DIR = f\"gs://{BUCKET_NAME}/{gcs_path}\"\n",
    "\n",
    "\n",
    "ENDPOINT_DISPLAY_NAME = 'fuel-endpoint'\n",
    "SERVING_CONTAINER_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'\n",
    "ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "\n",
    "\n",
    "LABELS = {\n",
    "    'creator': USER,\n",
    "    'workflow': 'fuel-prediction',\n",
    "    'type': 'regression'}\n",
    "\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_NAME,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    experiment_description=\"Fuel prediction pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8691e67-66c8-4021-80b4-af004b4619bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=PIPELINE_NAME\n",
    ")\n",
    "def pipeline(\n",
    "    gcs_uri: str,\n",
    "    dataset_name: str,\n",
    "    threshold_dict_str: str,\n",
    "    user:str,\n",
    "    managed_dataset_name: str,\n",
    "    endpoint_display_name: str,\n",
    "    endpoint_machine_type:str,\n",
    "    table_name: str,\n",
    "    project_id: str,\n",
    "    pipeline_name: str,\n",
    "    serving_container_image_uri:str,\n",
    "    model_name: str,\n",
    "    region: str):\n",
    "\n",
    "    # STEP 1:  Create bq table and dataset\n",
    "    create_bq_dataset_task = create_bq_dataset_op(\n",
    "        gcs_uri=gcs_uri,\n",
    "        project_id=project_id,\n",
    "        dataset_name=dataset_name,\n",
    "        table_name=table_name\n",
    "        ).set_caching_options(True) \\\n",
    "        .set_display_name('create-bq-dataset-table-op')\n",
    "    \n",
    "    \n",
    "    \n",
    "   # STEP 2: Prepare train and test datasets\n",
    "    prepare_data_op = prepare_datasets_op(\n",
    "        #bq_dataset=create_dataset_op.outputs['dataset']\n",
    "        bq_dataset =create_bq_dataset_task.outputs['bq_dataset_uri']\n",
    "    ).set_caching_options(True) \\\n",
    "        .set_display_name('prepare-datasets-op')\n",
    "\n",
    "    # STEP 3: Train and evaluate model\n",
    "    train_evaluate_model_task = train_evaluate_model_op(\n",
    "        dataset_train = prepare_data_op.outputs['dataset_train'],\n",
    "        dataset_test = prepare_data_op.outputs['dataset_test'],\n",
    "        threshold_dict_str=threshold_dict_str,\n",
    "        pipeline_name=pipeline_name\n",
    "    ).set_display_name('training-evaluation-job-op')\\\n",
    "        .set_caching_options(True)\n",
    "\n",
    "    ## Step3: Decision: If model performs according to our threshold, then deploy model and Enp\n",
    "    with dsl.Condition(\n",
    "            train_evaluate_model_task.outputs[\"dep_decision\"] == \"true\",\n",
    "            name=\"deploy_decision\",\n",
    "        ):\n",
    "            # Upload Model, Create a Vertex Endpoint resource and Deploy Model to Endpoint.\n",
    "\n",
    "            deploy_model_endpoint_op(\n",
    "                            model=train_evaluate_model_task.outputs['model'],\n",
    "                            region=region,\n",
    "                endpoint_display_name=endpoint_display_name,\n",
    "                            model_name=model_name,\n",
    "                            pipeline_name=pipeline_name,\n",
    "                            project_id=project_id,\n",
    "                machine_type=endpoint_machine_type,\n",
    "                model_display_name=model_name,\n",
    "                serving_container_image_uri=serving_container_image_uri\n",
    "            ).set_display_name('deploy-model-op').set_caching_options(True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6939c-1fa4-405f-8fbf-527e36966561",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997c21e-2436-4262-b606-cf693d2c1f44",
   "metadata": {},
   "source": [
    "### Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27876437-0828-44d2-b608-3751051fd2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=True,\n",
    "    labels=LABELS,\n",
    "    parameter_values={\n",
    "        'gcs_uri': DATA_GCS_DIR,\n",
    "        'dataset_name': MANAGED_DATASET_NAME,\n",
    "        'threshold_dict_str':THRESHOLD_DICT_STR,\n",
    "        'user': USER,\n",
    "        'managed_dataset_name': MANAGED_DATASET_NAME,\n",
    "        'endpoint_display_name': ENDPOINT_DISPLAY_NAME,\n",
    "        'endpoint_machine_type': ENDPOINT_MACHINE_TYPE,\n",
    "        'table_name': TABLE_NAME,\n",
    "        'project_id': PROJECT_ID,\n",
    "        'pipeline_name': PIPELINE_NAME,\n",
    "        'serving_container_image_uri': SERVING_CONTAINER_IMAGE_URI,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'region': REGION\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "job.submit(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9a2e5-4f76-4431-94d0-1d0ab6cf0d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7aae41-be11-4759-ba5a-c35474935d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d59fe796-61db-4780-9a53-6fef33f90596",
   "metadata": {},
   "source": [
    "## Online Predictions with Deployed Endpoint\n",
    "\n",
    "Retrieve the `Endpoint` deployed by the pipeline and use it to query your model for online predictions.\n",
    "\n",
    "Configure the `Endpoint()` function below with the following parameters:\n",
    "\n",
    "*  `endpoint_name`: A fully-qualified endpoint resource name or endpoint ID. Example: \"projects/123/locations/us-central1/endpoints/456\" or \"456\" when project and location are initialized or passed.\n",
    "*  `project_id`: GCP project.\n",
    "*  `location`: GCP region.\n",
    "\n",
    "Call `predict()` to return a prediction for a test review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "e1920256-da90-4813-bca1-99534e57c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_NAME = '9056039561246801920'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "084cb500-73bd-4f3c-9e92-9dacc7e11bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = vertexai.Endpoint(ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "fe2369f0-5962-4e58-b875-c4fcacc825ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = endpoint.predict([4,90.0,75.0,2125.0,14.5,74,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "a0ab35b6-171c-4577-bd59-62ce22636a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=[[26.7319756]], deployed_model_id='113350852730683392', model_version_id='1', model_resource_name='projects/525292812449/locations/us-central1/models/5413674197773713408', explanations=None)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d0f80-4b9e-4f5a-b39d-c7fc99c249c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26bb3105-67a4-4c1e-bbc3-e932bc714c5c",
   "metadata": {},
   "source": [
    "## Batch Predictions with Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466b477-b0aa-4c70-807b-da6d3f6ea278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch job args\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\") \n",
    "batch_job_display_name = \"fuel-batch-prediction-job\"\n",
    "gcs_batch_data = ''\n",
    "instances_format = 'csv'\n",
    "gcs_dest_results = f''\n",
    "machine_type = \"n1-standard-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa8c45-ae83-411c-b360-e223c40f4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ai models list --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fc5c8-cd36-4ec1-8535-e9dca9a710b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = ''\n",
    "MODEL_ID = !(gcloud ai models list --region=$REGION \\\n",
    "           --filter=display_name=$MODEL_NAME)\n",
    "MODEL_ID = MODEL_ID[2].split(\" \")[0]\n",
    "MODEL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece47377-f591-49e1-a0e8-bc42e2f9f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resource_name = f'projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}'\n",
    "model_resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec760737-e25c-4aad-bf6e-01e31f3a4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiplatform.init(project=project, location=location)\n",
    "model = aiplatform.Model(model_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84f440-2616-49ca-8327-3549fe286190",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=batch_job_display_name,\n",
    "        instances_format='csv', #json\n",
    "        gcs_source=[gcs_batch_data],\n",
    "        gcs_destination_prefix=gcs_dest_results,\n",
    "        machine_type=machine_type, # must be present      \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
