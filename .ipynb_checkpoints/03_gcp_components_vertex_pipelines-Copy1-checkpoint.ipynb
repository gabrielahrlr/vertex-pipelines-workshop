{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8184cc-3583-4187-b58b-02c48d08ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57301cb-c630-41ab-90a0-30442ea38e06",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 03 - Vertex Pipelines using  Google Cloud Pipeline Components\n",
    "\n",
    "## Overview \n",
    "\n",
    "This notebook shows how to use Kubeflow components to build a custom regression workflow on `Vertex AI Pipelines`.\n",
    "\n",
    "You will build a pipeline in this notebook that looks like this:\n",
    "\n",
    "<img src=\"img/pipelines-gcc.png\" width=\"90%\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Notebook Objective\n",
    "\n",
    "In this notebook, you will learn to use `Vertex AI Pipelines` with **ONLY** `Google Cloud Pipeline Components` to build a `custom` tabular regression model. In the pipeline we  will orchestrate data creation, data processing, model training and evaluation, and model deployment. We'll also see how to send payloads the endpoint deployed and how to run batch predition jobs.\n",
    "\n",
    "This lab uses the following Google Cloud services and resources:\n",
    "\n",
    "- `BigQuery`\n",
    "- `Vertex AI Pipelines`\n",
    "- `Google Cloud Pipeline Components`\n",
    "- `Vertex AI Model`\n",
    "- `Vertex AI Model Registry`\n",
    "- `Vertex AI Metadata`\n",
    "- `Vertex AI Endpoint`\n",
    "\n",
    "The steps performed in this notebook include:\n",
    "\n",
    "1. [Load Configuration settings from the setup notebook](#Load-Configuration-settings-from-the-setup-notebook)\n",
    "1. [Vertex Pipelines Introduction](#Vertex-Pipelines-Introduction)\n",
    "1. [Create a KFP Pipeline](#Create-a-KFP-Pipeline)\n",
    "    1. [Create a dataset in BigQuery](#Step-1:-Create-a-dataset-in-BigQuery)\n",
    "    1. [Transform the Data](#Step-2:-Transform-the-Data)\n",
    "    1. [Train and Evaluate our custom Regression Model](#Step-3:-Train-and-Evaluate-our-custom-Regression-Model)\n",
    "    1. [Upload Model to Vertex AI and Deploy Endpoint](#Step-4:-Upload-Model-to-Vertex-AI-and-Deploy-Endpoint)\n",
    "1. [Compile the KFP Pipeline](#Compile-the-KFP-Pipeline)\n",
    "1. [Execute the KFP Pipeline using Vertex AI Pipelines](#Execute-the-KFP-Pipeline-using-Vertex-AI-Pipelines)\n",
    "1. [Inspect Experiments](#)\n",
    "1. [Online Predictions](#)\n",
    "1. [Batch Preditions](#)\n",
    "\n",
    "\n",
    "The Google Cloud Components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform) \n",
    "\n",
    "### Dataset\n",
    "\n",
    "In this workshop we'll use the **public datase**t [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) for demonstration purposes. The data concerns city-cycle fuel consumption in miles per gallon, to be predicted in terms of 3 multivalued discrete and 5 continuous attributes. The objective will be to build a model to predict \"MPG\" (Miles per Gallon).\n",
    "\n",
    "Check notebook  `01_exploratory_data_analysis.ipynb` for further details of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f8fe7-3013-4671-b5ad-fc5931a6251f",
   "metadata": {},
   "source": [
    "## Load Configuration settings from the setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed77f6d-ebae-413d-b2dc-ddd0cd85252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our configurations from notebook 00_environment_setup.ipynb\n",
    "from src.config import config\n",
    "\n",
    "PROJECT_ID = config['PROJECT_ID']\n",
    "REGION = config['REGION']\n",
    "ID = config['ID']\n",
    "BUCKET_NAME = config['BUCKET_NAME']\n",
    "GCS_DATA_URI = config['GCS_DATA_URI']\n",
    "BQ_DATASET_URI = config['BQ_DATASET_URI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a110447-b83e-42c2-923d-36ea9d314329",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719f7ec-8eaf-4892-9eef-b920c3574ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Import the Vertex AI Python SDK \n",
    "from google.cloud import aiplatform as aip\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import google.auth\n",
    "from google.cloud import storage\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "# kfp sdk, to create the Vertex AI Pipelines\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, Model, Output, Metrics, ClassificationMetrics, component, OutputPath, InputPath)\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "\n",
    "# TensorFlow model building libraries.\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Custom Modules\n",
    "from src.helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb9ee9-013f-492d-99a5-2244a62eb43d",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92536907-36a6-4a7c-a96f-f5f589357180",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a9238-094d-45aa-849f-76170342ef45",
   "metadata": {},
   "source": [
    "## Create Custom Training Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417e3f43-0464-480a-8ccc-082522f4e321",
   "metadata": {},
   "source": [
    "Train and deploy your model on Google Cloud's Vertex AI platform.\n",
    "\n",
    "To train your BERT classifier on Google Cloud, you will you will package your Python training scripts and write a Dockerfile that contains instructions on your ML model code, dependencies, and execution instructions. You will build your custom container with Cloud Build, whose instructions are specified in `cloudbuild.yaml` and publish your container to your Artifact Registry. This workflow gives you the opportunity to use the same container to run as part of a portable and scalable [Vertex Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) workflow. \n",
    "\n",
    "\n",
    "You will walk through creating the following project structure for your ML mode code:\n",
    "```\n",
    "|--/container\n",
    "   |--/trainer\n",
    "      |--__init__.py\n",
    "      |--model.py\n",
    "      |--task.py\n",
    "   |--Dockerfile\n",
    "   |--cloudbuild.yaml\n",
    "   |--requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236ead7-5f66-49aa-9cce-a689afbe796c",
   "metadata": {},
   "source": [
    "### Step 1: Write model.py training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef801c6e-a6f5-4be9-aca3-7f05c348aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = f\"container\"\n",
    "MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ebd943-ec67-4b4c-a776-16727d2e81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {MODEL_DIR}/trainer/model.py\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "bqclient = bigquery.Client()\n",
    "storage_client = storage.Client()\n",
    "\n",
    "def download_table(bq_table_uri: str):\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bqclient.list_rows(\n",
    "        table,\n",
    "    )\n",
    "    \n",
    "    return rows.to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "\n",
    "def build_and_compile_model(norm):\n",
    "        model = keras.Sequential([\n",
    "            norm,\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "        return model\n",
    "\n",
    "def transform_data(df):\n",
    "    df.rename(columns = {\n",
    "        'mpg':'MPG',\n",
    "        'cyl':'Cylinders',\n",
    "        'dis':'Displacement',\n",
    "        'hp': 'Horsepower',\n",
    "        'weight': 'Weight',\n",
    "        'accel': 'Acceleration',\n",
    "        'year': 'Model Year',\n",
    "        'origin': 'Origin'}, inplace = True)\n",
    "\n",
    "    # Get data in shape\n",
    "    df = df.copy()\n",
    "    df.tail()\n",
    "    df = df.dropna()\n",
    "    df['Origin'] = df['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "    df = pd.get_dummies(df, columns=['Origin'], prefix='', prefix_sep='')\n",
    "    \n",
    "\n",
    "def train_model(params):\n",
    "    import logging\n",
    "    logging.info(f\"Training bq path {params['train-data-dir']}\")\n",
    "    logging.info(f\"Validation bq path {params['val-data-dir']}\")\n",
    "    train_dataset = download_table(params['train-data-dir'])\n",
    "    test_dataset = download_table(params['val-data-dir'])\n",
    "    \n",
    "    train_dataset = transform_data(df=train_dataset)\n",
    "    test_dataset = transform_data(df=test_dataset)\n",
    "    \n",
    "    train_features = train_dataset.copy()\n",
    "    train_labels = train_features.pop('MPG')\n",
    "    \n",
    "    test_features = test_dataset.copy()\n",
    "    test_labels = test_features.pop('MPG')\n",
    "\n",
    "    # Create model\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "    normalizer.adapt(np.array(train_features))\n",
    "    first = np.array(train_features[:1])\n",
    "    horsepower = np.array(train_features['Horsepower'])\n",
    "    horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
    "    horsepower_normalizer.adapt(horsepower)\n",
    "\n",
    "\n",
    "    dnn_model = build_and_compile_model(normalizer)\n",
    "    dnn_model.summary()\n",
    "\n",
    "    history = dnn_model.fit(\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        validation_split=0.2,\n",
    "        verbose=0, epochs=100\n",
    "    )\n",
    "\n",
    "    test_results = {}\n",
    "\n",
    "    test_results['dnn_model'] = dnn_model.evaluate(\n",
    "        test_features,\n",
    "        test_labels,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Log metrics\n",
    "    metrics_training = {metric: values[-1] for metric, values in history.history.items()}\n",
    "    metrics.log_metric('loss', metrics_training['loss'])\n",
    "    metrics.log_metric('val_loss', metrics_training['val_loss'])\n",
    "    model.uri = bucket\n",
    "    model.metadata['loss'] = metrics_training['loss']\n",
    "    model.metadata['val_loss'] = metrics_training['val_loss']\n",
    "    model.metadata['pipeline'] = pipeline_name\n",
    "\n",
    "    # Save the model to GCS\n",
    "    dnn_model.save(params['model-dir'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd809a74-4d1c-4766-9f6a-9bf93cfade1b",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979bf73-de29-43cb-83c5-8d487c620e82",
   "metadata": {},
   "source": [
    "### Step 2: Write a `task.py` file as an entrypoint to your custom model container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fb82fc-a97f-4969-8b08-88ca5607909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {MODEL_DIR}/trainer/task.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "    \n",
    "    \n",
    "\n",
    "from trainer import model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    logging.info(f\"Training bq path {os.environ['AIP_TRAINING_DATA_URI']}\")\n",
    "    logging.info(f\"Validation bq path {os.environ['AIP_VALIDATION_DATA_URI']}\")\n",
    "    # Vertex custom container training args. These are set by Vertex AI during training but can also be overwritten.\n",
    "    parser.add_argument('--model-dir', dest='model-dir',\n",
    "                        default=os.environ['AIP_MODEL_DIR'], type=str, help='GCS URI for saving model artifacts.')\n",
    "    \n",
    "    parser.add_argument('--train-data-dir', dest='train-data-dir',\n",
    "                        default=os.environ['AIP_TRAINING_DATA_URI'], type=str, help='BQ URI where the data is')    \n",
    "    \n",
    "    parser.add_argument('--val-data-dir', dest='val-data-dir',\n",
    "                        default=os.environ['AIP_VALIDATION_DATA_URI'], type=str, help='BQ URI where the data is')    \n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    params = args.__dict__\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.train_model(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6003e-4fc2-4319-a265-3c3ad94865a6",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79847a0-c64b-4184-81b8-4346764da5b3",
   "metadata": {},
   "source": [
    "### Step 3: Write a `Dockerfile` for your custom model container\n",
    "\n",
    "Third, you will write a `Dockerfile` that contains instructions to package your model code in `container` as well as specifies your model code's dependencies needed for execution together in a Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e33f1-6379-46dc-ac50-3e8c9f06c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {MODEL_DIR}/Dockerfile\n",
    "# Specifies base image and tag.\n",
    "# https://cloud.google.com/vertex-ai/docs/training/pre-built-containers\n",
    "FROM us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-6:latest\n",
    "\n",
    "# Sets the container working directory.\n",
    "WORKDIR /root\n",
    "\n",
    "# Copies the requirements.txt into the container to reduce network calls.\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Installs additional packages.\n",
    "RUN pip3 install -U -r requirements.txt\n",
    "\n",
    "# b/203105209 Removes unneeded file from TF2.5 CPU image for python_module CustomJob training. \n",
    "# Will be removed on subsequent public Vertex images.\n",
    "RUN rm -rf /var/sitecustomize/sitecustomize.py\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY . /trainer\n",
    "\n",
    "# Sets the container working directory.\n",
    "WORKDIR /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e836d4-4370-439e-935f-ba84b249b844",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82456c83-5b7e-495b-a776-59ed07a78430",
   "metadata": {},
   "source": [
    "### Step 4: Write a `requirements.txt` file to specify additional ML code dependencies\n",
    "\n",
    "These are additional dependencies for your model code not included in the pre-built Vertex TensorFlow images such as TensorFlow and google cloud sdks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c32d59d-8e87-495d-8a41-6a36a12c8a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {MODEL_DIR}/requirements.txt\n",
    "pandas\n",
    "google-cloud-storage\n",
    "google-cloud-bigquery==2.34.3\n",
    "protobuf==3.20.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784f8347-0bcc-470a-bbd3-151aa3726413",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152136f-bfbd-48fe-b128-c1d89c5c5a13",
   "metadata": {},
   "source": [
    "### Step 5: Use Cloud Build to build and submit your model container to Google Cloud Artifact Registry\n",
    "\n",
    "Next, you will use [Cloud Build](https://cloud.google.com/build) to build and upload your custom TensorFlow model container to [Google Cloud Artifact Registry](https://cloud.google.com/artifact-registry).\n",
    "\n",
    "Cloud Build brings reusability and automation to your ML experimentation by enabling you to reliably build, test, and deploy your ML model code as part of a CI/CD workflow. Artifact Registry provides a centralized repository for you to store, manage, and secure your ML container images. This will allow you to securely share your ML work with others and reproduce experiment results.\n",
    "\n",
    "**Note**: the initial build and submit step will take about 16 minutes but Cloud Build is able to take advantage of caching for faster subsequent builds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d4fd3-a21b-4614-962c-bb2c3330ff7d",
   "metadata": {},
   "source": [
    "####  5.1. Create Artifact Registry for custom container images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aed58d-41a9-465e-921c-9869613e7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_REGISTRY=\"fuel-regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3773f3d-6616-4b93-9aa5-244cfde80c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud artifacts repositories create $ARTIFACT_REGISTRY --location=us-central1 --repository-format=docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10101f8c-9d70-4aa4-97fc-ca08077853d7",
   "metadata": {},
   "source": [
    "#### 5.2. Create `cloudbuild.yaml` instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6d4c0-47eb-4fff-886b-14c53c67f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME=f\"fuel-regression\"\n",
    "IMAGE_TAG=\"latest\"\n",
    "IMAGE_URI=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{ARTIFACT_REGISTRY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf12447-3b52-4687-bec9-32513a7bc840",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudbuild_yaml = f\"\"\"steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: [ 'build', '-t', '{IMAGE_URI}', '.' ]\n",
    "images: \n",
    "- '{IMAGE_URI}'\"\"\"\n",
    "\n",
    "with open(f\"{MODEL_DIR}/cloudbuild.yaml\", \"w\") as fp:\n",
    "    fp.write(cloudbuild_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca1467-546d-4a92-a916-58e33c8ebe17",
   "metadata": {},
   "source": [
    "#### 5.3. Build and submit your container image to Artifact Registry using Cloud Build\n",
    "In the terminal do\n",
    "cd container and run below command\n",
    "\n",
    "```shell\n",
    "gcloud builds submit --region=us-central1 --config cloudbuild.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a02e455-11ab-4084-b08f-c7bd34334141",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b959a51-bc11-44aa-86fe-fe2a2048f8d7",
   "metadata": {},
   "source": [
    "## Vertex Pipelines Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27cd415-58cf-41a1-8b62-94827d6495bd",
   "metadata": {},
   "source": [
    "[Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner, and storing your workflow's artifacts using Vertex ML Metadata. By storing the artifacts of your ML workflow in Vertex ML Metadata, you can analyze the lineage of your workflow's artifacts â€” for example, an ML model's lineage may include the training data, hyperparameters, and code that were used to create the model.\n",
    "\n",
    "You can build your Pipelines using the battle-tested and easy-to-use `KubeFlow Pipelines (KFP) SDK` or ` TensorFlow Extended (TFX) SDK`. \n",
    "\n",
    "Within your Vertex Pipeline with `KubeFlow Pipelines (KFP) SDK` you can use either your own custom components using `KubeFlow Components` or already-built compontents using `Google Cloud Pipeline Components`.\n",
    "\n",
    "The Google Cloud Components are [documented here](https://google-cloud-pipeline-components.readthedocs.io/en/latest/google_cloud_pipeline_components.aiplatform.html#module-google_cloud_pipeline_components.aiplatform). \n",
    "\n",
    "The KubeFlow Compoenents are [documented here](https://www.kubeflow.org/docs/components/pipelines/v1/sdk-v2/python-function-components/)\n",
    "\n",
    "<img src=img/vertex-pipelines-def.png width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929a7e96-de39-4511-b51c-b2e84e9e7dfc",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48781f94-263c-49dd-b7f2-261b65084201",
   "metadata": {},
   "source": [
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561d229-ec9e-4e05-9086-6bdd533a009a",
   "metadata": {},
   "source": [
    "## Create a KFP Pipeline\n",
    "\n",
    "To address your business requirements and get your higher performing model into production to deliver value faster, you will define a pipeline using the [**Kubeflow Pipelines (KFP) V2 SDK**](https://www.kubeflow.org/docs/components/pipelines/sdk/v2/v2-compatibility) to orchestrate the training and deployment of your model on [**Vertex Pipelines**](https://cloud.google.com/vertex-ai/docs/pipelines) below.\n",
    "\n",
    "The pipeline consists of four `Google Cloud Custom Components`:\n",
    "\n",
    "* `TabularDatasetCreateOp`[(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.TabularDatasetCreateOp): Creates a Tabular Managed Dataset in Vertex AI Datasets.\n",
    "\n",
    "* `CustomContainerTrainingJobRunOp` [(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.CustomContainerTrainingJobRunOp): trains your custom model container using Vertex Training. This is the same as configuring a Vertex Custom Container Training Job using the Vertex Python SDK you covered in the Vertex AI: Qwik Start lab.\n",
    "\n",
    "*  `EndpointCreateOp` [(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.EndpointCreateOp): Creates a Google Cloud Vertex Endpoint resource that maps physical machine resources with your model to enable it to serve online predictions. Online predictions have low latency requirements; providing resources to the model in advance reduces latency. \n",
    "\n",
    "* `ModelDeployOp`[(documentation)](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-0.2.0/google_cloud_pipeline_components.aiplatform.html#google_cloud_pipeline_components.aiplatform.ModelDeployOp): deploys your model to a Vertex Prediction Endpoint for online predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd4b7c-1b4f-47f3-9105-b638651ecf12",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a73721-eee4-4f0c-a772-76583d451090",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compile the KFP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e884bf3-5834-4374-9695-35a44479f947",
   "metadata": {},
   "source": [
    "#### Define Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bb60e-dac5-4e6b-a72f-86d807717463",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "USER = 'add-your-name-lowercase'\n",
    "USER = 'gabriela'\n",
    "if USER == 'add-your-name-lowercase':\n",
    "    USER = 'unknown'\n",
    "\n",
    "\n",
    "EXPERIMENT_NAME = \"fuel-model-google-cloud-components\"\n",
    "EXPERIMENT_DESCRIPTION = \"Fuel prediction pipeline usign Google Cloud Components and Custom Container\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/{USER}\"\n",
    "\n",
    "## Important the pipeline name always has the timestamp as part of the name.\n",
    "PIPELINE_NAME = f\"{EXPERIMENT_NAME}-{TIMESTAMP}\"\n",
    "PIPELINE_PACKAGE_PATH = f'{PIPELINE_NAME}-path.json'\n",
    "\n",
    "\n",
    "MODEL_NAME = EXPERIMENT_NAME\n",
    "DEPLOY_ENDPOINT = \"True\"\n",
    "\n",
    "MANAGED_DATASET_NAME = 'fuel_dataset'\n",
    "MODEL_NAME = 'fuel-prediction'\n",
    "BQ_DATA_DIR = BQ_DATASET_URI\n",
    "\n",
    "ENDPOINT_DISPLAY_NAME = 'fuel-endpoint'\n",
    "SERVING_CONTAINER_IMAGE_URI = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'\n",
    "ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "LABELS = {\n",
    "    'creator': USER,\n",
    "    'workflow': 'fuel-prediction',\n",
    "    'type': 'regression'}\n",
    "\n",
    "\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=BUCKET_NAME,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    "    experiment_description=\"Fuel prediction pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8691e67-66c8-4021-80b4-af004b4619bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "@dsl.pipeline(\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    name=PIPELINE_NAME\n",
    ")\n",
    "def pipeline(\n",
    "    bq_dataset_uri: str,\n",
    "    managed_dataset_name: str,\n",
    "    container_uri: str,\n",
    "    staging_bucket:str,\n",
    "    bq_destination: str,\n",
    "    deploy_endpoint:str,\n",
    "    pipeline_name: str,\n",
    "    endpoint_display_name: str,\n",
    "    endpoint_machine_type:str,\n",
    "    model_name: str,\n",
    "    model_serving_container_image_uri: str,\n",
    "    region: str,\n",
    "    project_id: str\n",
    "):\n",
    "\n",
    "\n",
    "    # STEP 1: Create Managed Dataset\n",
    "    \n",
    "    create_dataset_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        display_name=managed_dataset_name,\n",
    "        bq_source=bq_dataset_uri,\n",
    "        project=project_id,\n",
    "        location=region\n",
    "    ).set_caching_options(True) \\\n",
    "        .set_display_name('create-managed-dataset-op')\n",
    "    \n",
    "    ## STEP 2: Create Training Job\n",
    "    training_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        display_name=\"pipeline-fuel-custom-train\",\n",
    "        container_uri=container_uri,\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        dataset=create_dataset_op.outputs[\"dataset\"],\n",
    "        staging_bucket=staging_bucket,\n",
    "        training_fraction_split=0.7,\n",
    "        validation_fraction_split=0.2,\n",
    "        test_fraction_split=0.1,\n",
    "        bigquery_destination=bq_destination,\n",
    "        model_serving_container_image_uri=model_serving_container_image_uri,\n",
    "        model_display_name=\"fuel-custom-model-pipeline\",\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )\n",
    "    \n",
    "\n",
    "    ## Step3: Decision: If model performs according to our threshold, then deploy model and Enp\n",
    "    with dsl.Condition(\n",
    "            deploy_endpoint == \"true\",\n",
    "            name=\"deploy_decision\",\n",
    "        ):\n",
    "        ## Step 4: Create Endpoint\n",
    "        endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            display_name=endpoint_display_name,\n",
    "        ).set_display_name('create-endpoint-op').set_caching_options(True)\n",
    "\n",
    "        ## Step 5: Deploy Model To Endpoint\n",
    "        gcc_aip.ModelDeployOp(\n",
    "            model=training_op.outputs[\"model\"],\n",
    "            endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            dedicated_resources_machine_type=endpoint_machine_type\n",
    "        ).set_display_name('deploy-model-op').set_caching_options(True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6939c-1fa4-405f-8fbf-527e36966561",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=PIPELINE_PACKAGE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ec150-96ef-4f89-8507-4378c58e1f19",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997c21e-2436-4262-b606-cf693d2c1f44",
   "metadata": {},
   "source": [
    "### Execute the KFP Pipeline using Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27876437-0828-44d2-b608-3751051fd2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=PIPELINE_PACKAGE_PATH,\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=True,\n",
    "    labels=LABELS,\n",
    "    parameter_values={\n",
    "        'bq_dataset_uri': BQ_DATA_DIR,\n",
    "        'managed_dataset_name': MANAGED_DATASET_NAME,\n",
    "        'container_uri': IMAGE_URI,\n",
    "        'staging_bucket': BUCKET_NAME,\n",
    "        'bq_destination':f\"bq://{PROJECT_ID}\",\n",
    "        'deploy_endpoint': DEPLOY_ENDPOINT,\n",
    "        'pipeline_name': PIPELINE_NAME,\n",
    "        'endpoint_display_name': ENDPOINT_DISPLAY_NAME,\n",
    "        'endpoint_machine_type': ENDPOINT_MACHINE_TYPE,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'model_serving_container_image_uri': SERVING_CONTAINER_IMAGE_URI,\n",
    "        'project_id': PROJECT_ID,\n",
    "        'region': REGION\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit(experiment=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b36214-0ad4-4459-85f9-917e957404ca",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57f6e2-a4d8-40e4-bdc3-51f1f47a1a43",
   "metadata": {},
   "source": [
    "## Inspect Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3bf9aa-d771-497b-acce-5e2f7efa5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experiments_data(\n",
    "  experiment_name: str,\n",
    "  project: str,\n",
    "  location: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Get experiments\n",
    "    \"\"\"\n",
    "    aip.init(experiment=experiment_name, project=project, location=location)\n",
    "    experiments_df = aip.get_experiment_df()\n",
    "    return experiments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcf265-bd3f-41f3-a8fd-67a800c0730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_experiments_data(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5736899-a9fa-461d-801c-bbb8a1232b68",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fe796-61db-4780-9a53-6fef33f90596",
   "metadata": {},
   "source": [
    "## Online Predictions with Deployed Endpoint\n",
    "\n",
    "Retrieve the `Endpoint` deployed by the pipeline and use it to query your model for online predictions.\n",
    "\n",
    "Configure the `Endpoint()` function below with the following parameters:\n",
    "\n",
    "*  `endpoint_name`: A fully-qualified endpoint resource name or endpoint ID. Example: \"projects/123/locations/us-central1/endpoints/456\" or \"456\" when project and location are initialized or passed.\n",
    "*  `project_id`: GCP project.\n",
    "*  `location`: GCP region.\n",
    "\n",
    "Call `predict()` to return a prediction for a test review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1920256-da90-4813-bca1-99534e57c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID = 'insert-your-endpoint-id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084cb500-73bd-4f3c-9e92-9dacc7e11bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = vertexai.Endpoint(ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2369f0-5962-4e58-b875-c4fcacc825ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = endpoint.predict([4,90.0,75.0,2125.0,14.5,74,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab35b6-171c-4577-bd59-62ce22636a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0de80f-84f6-42fe-9504-a6aad0009924",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb3105-67a4-4c1e-bbc3-e932bc714c5c",
   "metadata": {},
   "source": [
    "## Batch Predictions with Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857f07b-4f47-458b-b16e-7a44147772d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a fake batch file in Cloud Storage by randomly sampling our dataset\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(GCS_DATA_URI, header=None)\n",
    "# Remove label\n",
    "dataset = dataset.iloc[:,1:]\n",
    "\n",
    "batch_data = dataset.sample(10)\n",
    "batch_data.to_csv('data/batch_data_ex.csv', index=False)\n",
    "batch_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c12614-d9cc-4e92-953a-a6c88f0eb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload data to Cloud Storage\n",
    "from src.helper import upload_file_to_gcs\n",
    "\n",
    "gcs_batch_input_data_path = upload_file_to_gcs(\n",
    "    project_id=PROJECT_ID,\n",
    "    target=BUCKET_NAME,\n",
    "    source='data/batch_data_ex.csv',\n",
    "    blob_name=f'data/batch_prediction/input_data/fuel_data_{TIMESTAMP}.csv')\n",
    "gcs_batch_input_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466b477-b0aa-4c70-807b-da6d3f6ea278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch job args\n",
    "TIMESTAMP =datetime.now().strftime(\"%Y%m%d%H%M%S\") \n",
    "batch_job_display_name = \"fuel-batch-prediction-job\"\n",
    "gcs_batch_data = gcs_batch_input_data_path\n",
    "instances_format = 'csv'\n",
    "gcs_dest_results = f'gs://{BUCKET_NAME}/batch_jobs/output/{TIMESTAMP}/'\n",
    "machine_type = \"n1-standard-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa8c45-ae83-411c-b360-e223c40f4e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List all Models and pick the Model ID \n",
    "!gcloud ai models list --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097fc5c8-cd36-4ec1-8535-e9dca9a710b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'insert-your-model-id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece47377-f591-49e1-a0e8-bc42e2f9f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resource_name = f'projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}'\n",
    "model_resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec760737-e25c-4aad-bf6e-01e31f3a4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aiplatform.init(project=project, location=location)\n",
    "model = aip.Model(model_resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84f440-2616-49ca-8327-3549fe286190",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=batch_job_display_name,\n",
    "        instances_format='csv', #json\n",
    "        gcs_source=[gcs_batch_data],\n",
    "        gcs_destination_prefix=gcs_dest_results,\n",
    "        machine_type=machine_type, # must be present      \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123dccc3-0ff0-461b-88b3-9aa5fa1de694",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81c780-5857-4837-9668-a3682daa3975",
   "metadata": {},
   "source": [
    "# IMPORTANT! CLEAN UP ALL RESOURCES CREATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a15cb4a-a74b-4985-bfab-8be3dbabcec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
